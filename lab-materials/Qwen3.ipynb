{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1EPUyT0VE4S_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vllm huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbVp9RLjE8Au"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-04 03:09:19 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-04 03:09:24 [api_server.py:1043] vLLM API server version 0.8.5.post1\n",
      "INFO 05-04 03:09:24 [api_server.py:1044] args: Namespace(subparser='serve', model_tag='Qwen/Qwen3-14B', config='', host=None, port=8989, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='alanliuxiang', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=True, tool_call_parser='granite', tool_parser_plugin='', model='Qwen/Qwen3-14B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', max_model_len=34688, guided_decoding_backend='auto', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.8, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7f0669ef3ec0>)\n",
      "INFO 05-04 03:09:30 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'classify', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 05-04 03:09:30 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "INFO 05-04 03:09:30 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 05-04 03:09:34 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-04 03:09:37 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=34688, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-14B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-04 03:09:37 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 05-04 03:09:37 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_61abb095'), local_subscribe_addr='ipc:///tmp/4ce45209-0344-4755-825f-5d3c63d61d8a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 05-04 03:09:39 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-04 03:09:39 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-04 03:09:39 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-04 03:09:40 [__init__.py:239] Automatically detected platform cuda.\n",
      "WARNING 05-04 03:09:42 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb8cf072f50>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=360)\u001b[0;0m INFO 05-04 03:09:42 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_80e70883'), local_subscribe_addr='ipc:///tmp/a6011bc6-3b97-4df7-bd4c-c0dd6d60efe7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 05-04 03:09:42 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fcddbbf1410>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m INFO 05-04 03:09:42 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_217e4304'), local_subscribe_addr='ipc:///tmp/51d4ce7e-3c0f-4908-bd89-021a2b9bbe1f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 05-04 03:09:42 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f34fbb98dd0>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=359)\u001b[0;0m INFO 05-04 03:09:42 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9ee60170'), local_subscribe_addr='ipc:///tmp/11af90b1-d56d-492d-ad66-9a878e32bc43', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 05-04 03:09:42 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f15112501d0>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=358)\u001b[0;0m INFO 05-04 03:09:42 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_83ce9b1f'), local_subscribe_addr='ipc:///tmp/bbde5e2d-7b03-446e-9aea-816483089991', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m INFO 05-04 03:09:44 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m INFO 05-04 03:09:44 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=359)\u001b[0;0m INFO 05-04 03:09:44 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=358)\u001b[0;0m INFO 05-04 03:09:44 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=359)\u001b[0;0m INFO 05-04 03:09:44 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=358)\u001b[0;0m INFO 05-04 03:09:44 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=360)\u001b[0;0m INFO 05-04 03:09:44 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=360)\u001b[0;0m INFO 05-04 03:09:44 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:27<03:15, 27.96s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:58<02:55, 29.29s/it]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [01:28<02:28, 29.73s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [01:42<01:34, 23.68s/it]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [02:13<01:18, 26.08s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [02:43<00:55, 27.52s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [03:13<00:28, 28.30s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [03:43<00:00, 29.01s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [03:43<00:00, 27.99s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m \n",
      "\u001b[1;36m(VllmWorker rank=1 pid=358)\u001b[0;0m INFO 05-04 03:13:28 [loader.py:458] Loading weights took 224.22 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m INFO 05-04 03:13:28 [loader.py:458] Loading weights took 223.96 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=359)\u001b[0;0m INFO 05-04 03:13:29 [loader.py:458] Loading weights took 224.12 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=360)\u001b[0;0m INFO 05-04 03:13:29 [loader.py:458] Loading weights took 224.23 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=358)\u001b[0;0m INFO 05-04 03:13:29 [gpu_model_runner.py:1347] Model loading took 6.9455 GiB and 224.509786 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m INFO 05-04 03:13:29 [gpu_model_runner.py:1347] Model loading took 6.9455 GiB and 224.508049 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=360)\u001b[0;0m INFO 05-04 03:13:29 [gpu_model_runner.py:1347] Model loading took 6.9455 GiB and 224.579418 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=359)\u001b[0;0m INFO 05-04 03:13:29 [gpu_model_runner.py:1347] Model loading took 6.9455 GiB and 224.566177 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m INFO 05-04 03:13:40 [backends.py:420] Using cache directory: /opt/app-root/src/.cache/vllm/torch_compile_cache/4b136d2777/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m INFO 05-04 03:13:40 [backends.py:430] Dynamo bytecode transform time: 10.86 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=360)\u001b[0;0m INFO 05-04 03:13:40 [backends.py:420] Using cache directory: /opt/app-root/src/.cache/vllm/torch_compile_cache/4b136d2777/rank_3_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=360)\u001b[0;0m INFO 05-04 03:13:40 [backends.py:430] Dynamo bytecode transform time: 10.86 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=359)\u001b[0;0m INFO 05-04 03:13:40 [backends.py:420] Using cache directory: /opt/app-root/src/.cache/vllm/torch_compile_cache/4b136d2777/rank_2_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=359)\u001b[0;0m INFO 05-04 03:13:40 [backends.py:430] Dynamo bytecode transform time: 10.95 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=358)\u001b[0;0m INFO 05-04 03:13:40 [backends.py:420] Using cache directory: /opt/app-root/src/.cache/vllm/torch_compile_cache/4b136d2777/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=358)\u001b[0;0m INFO 05-04 03:13:40 [backends.py:430] Dynamo bytecode transform time: 10.97 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=360)\u001b[0;0m INFO 05-04 03:13:49 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 8.370 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m INFO 05-04 03:13:49 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 8.479 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=358)\u001b[0;0m INFO 05-04 03:13:49 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 8.470 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=359)\u001b[0;0m INFO 05-04 03:13:49 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 8.504 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m INFO 05-04 03:13:50 [monitor.py:33] torch.compile takes 10.86 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=360)\u001b[0;0m INFO 05-04 03:13:50 [monitor.py:33] torch.compile takes 10.86 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=358)\u001b[0;0m INFO 05-04 03:13:50 [monitor.py:33] torch.compile takes 10.97 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=359)\u001b[0;0m INFO 05-04 03:13:50 [monitor.py:33] torch.compile takes 10.95 s in total\n",
      "INFO 05-04 03:13:52 [kv_cache_utils.py:634] GPU KV cache size: 230,960 tokens\n",
      "INFO 05-04 03:13:52 [kv_cache_utils.py:637] Maximum concurrency for 34,688 tokens per request: 6.66x\n",
      "INFO 05-04 03:13:52 [kv_cache_utils.py:634] GPU KV cache size: 230,960 tokens\n",
      "INFO 05-04 03:13:52 [kv_cache_utils.py:637] Maximum concurrency for 34,688 tokens per request: 6.66x\n",
      "INFO 05-04 03:13:52 [kv_cache_utils.py:634] GPU KV cache size: 230,960 tokens\n",
      "INFO 05-04 03:13:52 [kv_cache_utils.py:637] Maximum concurrency for 34,688 tokens per request: 6.66x\n",
      "INFO 05-04 03:13:52 [kv_cache_utils.py:634] GPU KV cache size: 230,960 tokens\n",
      "INFO 05-04 03:13:52 [kv_cache_utils.py:637] Maximum concurrency for 34,688 tokens per request: 6.66x\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=359)\u001b[0;0m INFO 05-04 03:14:24 [gpu_model_runner.py:1686] Graph capturing finished in 32 secs, took 1.40 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=360)\u001b[0;0m INFO 05-04 03:14:24 [gpu_model_runner.py:1686] Graph capturing finished in 32 secs, took 1.40 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=358)\u001b[0;0m INFO 05-04 03:14:24 [gpu_model_runner.py:1686] Graph capturing finished in 32 secs, took 1.40 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=357)\u001b[0;0m INFO 05-04 03:14:24 [gpu_model_runner.py:1686] Graph capturing finished in 32 secs, took 1.40 GiB\n",
      "INFO 05-04 03:14:24 [core.py:159] init engine (profile, create kv cache, warmup model) took 55.18 seconds\n",
      "INFO 05-04 03:14:24 [core_client.py:439] Core engine process 0 ready.\n",
      "INFO 05-04 03:14:24 [serving_chat.py:80] \"auto\" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.\n",
      "WARNING 05-04 03:14:24 [config.py:1239] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
      "INFO 05-04 03:14:24 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "INFO 05-04 03:14:24 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\n",
      "INFO 05-04 03:14:24 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:8989\n",
      "INFO 05-04 03:14:24 [launcher.py:28] Available routes are:\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /docs, Methods: HEAD, GET\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /health, Methods: GET\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /load, Methods: GET\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /ping, Methods: POST, GET\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /tokenize, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /detokenize, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /v1/models, Methods: GET\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /version, Methods: GET\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /pooling, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /score, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /v1/score, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /rerank, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /invocations, Methods: POST\n",
      "INFO 05-04 03:14:24 [launcher.py:36] Route: /metrics, Methods: GET\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m177\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:53954 - \"\u001b[1mGET /v1/models HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-04 03:34:44 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "INFO 05-04 03:34:44 [logger.py:39] Received request chatcmpl-5d4472372add4dca99d3263dc566036b: prompt: '<|im_start|>system\\nYou are a helpful assistant\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"knowledge_search\", \"description\": \"Search for information in a database.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"The query to search for. Can be a natural language sentence or keywords.\"}}, \"required\": [\"query\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nHow to optimize memory usage in torchtune? use the knowledge_search tool to get information.<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 05-04 03:34:44 [async_llm.py:252] Added request chatcmpl-5d4472372add4dca99d3263dc566036b.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:40676 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-04 03:34:54 [loggers.py:111] Engine 000: Avg prompt throughput: 19.1 tokens/s, Avg generation throughput: 20.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
      "INFO 05-04 03:35:04 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
      "INFO 05-04 03:45:50 [logger.py:39] Received request chatcmpl-e7094528ef9d48fab9b2993bbb616964: prompt: '<|im_start|>system\\nYou are a helpful assistant. Use tools to answer. When you use a tool always respond with a summary of the result.\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"getOpportunities\", \"description\": \"Get a list of active opportunities from the CRM system\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getSupportCases\", \"description\": \"Get a list of support cases\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nGet active opportunities for acme<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 05-04 03:45:50 [async_llm.py:252] Added request chatcmpl-e7094528ef9d48fab9b2993bbb616964.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:41402 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-04 03:45:53 [logger.py:39] Received request chatcmpl-b0fdf2a7e9c049ffbcd0eee6e6e7c06f: prompt: '<|im_start|>system\\nYou are a helpful AI assistant, responsible for helping me find and communicate information back to my team.\\n    You have access to a number of tools.\\n    Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\\n    When you are asked to find out about opportunities and support casescounts you must use a tool.\\n    If you need to create a pdf, add this markdown to the start of the content:  ![ParasolCloud Logo](https://i.postimg.cc/MHZB5tmL/Screenshot-2025-04-21-at-5-58-46-PM.png)\\n*Secure Cloud Solutions for a Brighter Business*  \\n---\\n    \\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"getOpportunities\", \"description\": \"Get a list of active opportunities from the CRM system\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getSupportCases\", \"description\": \"Get a list of support cases\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"create_pdf_from_markdown\", \"description\": \"Convert markdown content to PDF. Note: Cannot handle LaTeX math equations. Supports basic markdown elements like headers, lists, tables, code blocks, blockquotes, and images (both local and external URLs).\", \"parameters\": {\"type\": \"object\", \"properties\": {\"markdown\": {\"type\": \"string\", \"description\": \"Markdown content to convert to PDF\"}, \"outputFilename\": {\"type\": \"string\", \"description\": \"The filename of the PDF file to be saved (e.g. \\\\\"output.pdf\\\\\"). The environmental variable M2P_OUTPUT_DIR sets the output path directory. If not provided, it will default to user\\'s HOME directory.\"}, \"paperFormat\": {\"type\": \"string\", \"description\": \"Paper format for the PDF (default: letter)\"}, \"paperOrientation\": {\"type\": \"string\", \"description\": \"Paper orientation for the PDF (default: portrait)\"}, \"paperBorder\": {\"type\": \"string\", \"description\": \"Border margin for the PDF (default: 2cm). Use CSS units (cm, mm, in, px)\"}, \"watermark\": {\"type\": \"string\", \"description\": \"Optional watermark text (max 15 characters, uppercase), e.g. \\\\\"DRAFT\\\\\", \\\\\"PRELIMINARY\\\\\", \\\\\"CONFIDENTIAL\\\\\", \\\\\"FOR REVIEW\\\\\", etc\"}}, \"required\": [\"markdown\", \"outputFilename\", \"paperFormat\", \"paperOrientation\", \"paperBorder\", \"watermark\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nGet active opportunities for ACME<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 05-04 03:45:53 [async_llm.py:252] Added request chatcmpl-b0fdf2a7e9c049ffbcd0eee6e6e7c06f.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:41402 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-04 03:45:54 [loggers.py:111] Engine 000: Avg prompt throughput: 88.6 tokens/s, Avg generation throughput: 12.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%\n",
      "INFO 05-04 03:46:00 [logger.py:39] Received request chatcmpl-edee4bd0c2044cf28c70efbe8b2a21af: prompt: '<|im_start|>system\\nYou are a helpful AI assistant, responsible for helping me find and communicate information back to my team.\\n    You have access to a number of tools.\\n    Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\\n    When you are asked to find out about opportunities and support casescounts you must use a tool.\\n    If you need to create a pdf, add this markdown to the start of the content:  ![ParasolCloud Logo](https://i.postimg.cc/MHZB5tmL/Screenshot-2025-04-21-at-5-58-46-PM.png)\\n*Secure Cloud Solutions for a Brighter Business*  \\n---\\n    \\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"getOpportunities\", \"description\": \"Get a list of active opportunities from the CRM system\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getSupportCases\", \"description\": \"Get a list of support cases\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"create_pdf_from_markdown\", \"description\": \"Convert markdown content to PDF. Note: Cannot handle LaTeX math equations. Supports basic markdown elements like headers, lists, tables, code blocks, blockquotes, and images (both local and external URLs).\", \"parameters\": {\"type\": \"object\", \"properties\": {\"markdown\": {\"type\": \"string\", \"description\": \"Markdown content to convert to PDF\"}, \"outputFilename\": {\"type\": \"string\", \"description\": \"The filename of the PDF file to be saved (e.g. \\\\\"output.pdf\\\\\"). The environmental variable M2P_OUTPUT_DIR sets the output path directory. If not provided, it will default to user\\'s HOME directory.\"}, \"paperFormat\": {\"type\": \"string\", \"description\": \"Paper format for the PDF (default: letter)\"}, \"paperOrientation\": {\"type\": \"string\", \"description\": \"Paper orientation for the PDF (default: portrait)\"}, \"paperBorder\": {\"type\": \"string\", \"description\": \"Border margin for the PDF (default: 2cm). Use CSS units (cm, mm, in, px)\"}, \"watermark\": {\"type\": \"string\", \"description\": \"Optional watermark text (max 15 characters, uppercase), e.g. \\\\\"DRAFT\\\\\", \\\\\"PRELIMINARY\\\\\", \\\\\"CONFIDENTIAL\\\\\", \\\\\"FOR REVIEW\\\\\", etc\"}}, \"required\": [\"markdown\", \"outputFilename\", \"paperFormat\", \"paperOrientation\", \"paperBorder\", \"watermark\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nGet active opportunities for ACME<|im_end|>\\n<|im_start|>assistant\\n<tool_call>\\n{\"name\": \"getOpportunities\", \"arguments\": {}}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nget a list of support cases for the account id<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 05-04 03:46:00 [async_llm.py:252] Added request chatcmpl-edee4bd0c2044cf28c70efbe8b2a21af.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:41402 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-04 03:46:04 [loggers.py:111] Engine 000: Avg prompt throughput: 70.3 tokens/s, Avg generation throughput: 31.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 38.0%\n",
      "INFO 05-04 03:46:08 [logger.py:39] Received request chatcmpl-3cded816b773424fb0aa1bac4b66058e: prompt: '<|im_start|>system\\nYou are a helpful AI assistant, responsible for helping me find and communicate information back to my team.\\n    You have access to a number of tools.\\n    Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\\n    When you are asked to find out about opportunities and support casescounts you must use a tool.\\n    If you need to create a pdf, add this markdown to the start of the content:  ![ParasolCloud Logo](https://i.postimg.cc/MHZB5tmL/Screenshot-2025-04-21-at-5-58-46-PM.png)\\n*Secure Cloud Solutions for a Brighter Business*  \\n---\\n    \\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"getOpportunities\", \"description\": \"Get a list of active opportunities from the CRM system\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getSupportCases\", \"description\": \"Get a list of support cases\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"create_pdf_from_markdown\", \"description\": \"Convert markdown content to PDF. Note: Cannot handle LaTeX math equations. Supports basic markdown elements like headers, lists, tables, code blocks, blockquotes, and images (both local and external URLs).\", \"parameters\": {\"type\": \"object\", \"properties\": {\"markdown\": {\"type\": \"string\", \"description\": \"Markdown content to convert to PDF\"}, \"outputFilename\": {\"type\": \"string\", \"description\": \"The filename of the PDF file to be saved (e.g. \\\\\"output.pdf\\\\\"). The environmental variable M2P_OUTPUT_DIR sets the output path directory. If not provided, it will default to user\\'s HOME directory.\"}, \"paperFormat\": {\"type\": \"string\", \"description\": \"Paper format for the PDF (default: letter)\"}, \"paperOrientation\": {\"type\": \"string\", \"description\": \"Paper orientation for the PDF (default: portrait)\"}, \"paperBorder\": {\"type\": \"string\", \"description\": \"Border margin for the PDF (default: 2cm). Use CSS units (cm, mm, in, px)\"}, \"watermark\": {\"type\": \"string\", \"description\": \"Optional watermark text (max 15 characters, uppercase), e.g. \\\\\"DRAFT\\\\\", \\\\\"PRELIMINARY\\\\\", \\\\\"CONFIDENTIAL\\\\\", \\\\\"FOR REVIEW\\\\\", etc\"}}, \"required\": [\"markdown\", \"outputFilename\", \"paperFormat\", \"paperOrientation\", \"paperBorder\", \"watermark\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nGet active opportunities for ACME<|im_end|>\\n<|im_start|>assistant\\n<tool_call>\\n{\"name\": \"getOpportunities\", \"arguments\": {}}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nget a list of support cases for the account id<|im_end|>\\n<|im_start|>assistant\\nThe function `getSupportCases` does not currently support filtering by account ID. Would you like me to retrieve all support cases, or is there another way I can assist you with this request?<|im_end|>\\n<|im_start|>user\\nAnalyze the support cases and determine how happy the account is<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 05-04 03:46:08 [async_llm.py:252] Added request chatcmpl-3cded816b773424fb0aa1bac4b66058e.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:41402 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-04 03:46:14 [loggers.py:111] Engine 000: Avg prompt throughput: 76.4 tokens/s, Avg generation throughput: 31.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 54.2%\n",
      "INFO 05-04 03:46:24 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 54.2%\n",
      "INFO 05-04 03:46:27 [logger.py:39] Received request chatcmpl-8a75f67bc7964736bd972623edc43051: prompt: '<|im_start|>system\\nYou are a helpful AI assistant, responsible for helping me find and communicate information back to my team.\\n    You have access to a number of tools.\\n    Whenever a tool is called, be sure return the Response in a friendly and helpful tone.\\n    When you are asked to find out about opportunities and support casescounts you must use a tool.\\n    If you need to create a pdf, add this markdown to the start of the content:  ![ParasolCloud Logo](https://i.postimg.cc/MHZB5tmL/Screenshot-2025-04-21-at-5-58-46-PM.png)\\n*Secure Cloud Solutions for a Brighter Business*  \\n---\\n    \\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"getOpportunities\", \"description\": \"Get a list of active opportunities from the CRM system\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getSupportCases\", \"description\": \"Get a list of support cases\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"create_pdf_from_markdown\", \"description\": \"Convert markdown content to PDF. Note: Cannot handle LaTeX math equations. Supports basic markdown elements like headers, lists, tables, code blocks, blockquotes, and images (both local and external URLs).\", \"parameters\": {\"type\": \"object\", \"properties\": {\"markdown\": {\"type\": \"string\", \"description\": \"Markdown content to convert to PDF\"}, \"outputFilename\": {\"type\": \"string\", \"description\": \"The filename of the PDF file to be saved (e.g. \\\\\"output.pdf\\\\\"). The environmental variable M2P_OUTPUT_DIR sets the output path directory. If not provided, it will default to user\\'s HOME directory.\"}, \"paperFormat\": {\"type\": \"string\", \"description\": \"Paper format for the PDF (default: letter)\"}, \"paperOrientation\": {\"type\": \"string\", \"description\": \"Paper orientation for the PDF (default: portrait)\"}, \"paperBorder\": {\"type\": \"string\", \"description\": \"Border margin for the PDF (default: 2cm). Use CSS units (cm, mm, in, px)\"}, \"watermark\": {\"type\": \"string\", \"description\": \"Optional watermark text (max 15 characters, uppercase), e.g. \\\\\"DRAFT\\\\\", \\\\\"PRELIMINARY\\\\\", \\\\\"CONFIDENTIAL\\\\\", \\\\\"FOR REVIEW\\\\\", etc\"}}, \"required\": [\"markdown\", \"outputFilename\", \"paperFormat\", \"paperOrientation\", \"paperBorder\", \"watermark\"]}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nGet active opportunities for ACME<|im_end|>\\n<|im_start|>assistant\\n<tool_call>\\n{\"name\": \"getOpportunities\", \"arguments\": {}}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nget a list of support cases for the account id<|im_end|>\\n<|im_start|>assistant\\nThe function `getSupportCases` does not currently support filtering by account ID. Would you like me to retrieve all support cases, or is there another way I can assist you with this request?<|im_end|>\\n<|im_start|>user\\nAnalyze the support cases and determine how happy the account is<|im_end|>\\n<|im_start|>assistant\\nTo analyze support cases and determine account happiness, I first need to retrieve the relevant support cases. However, the current `getSupportCases` tool does not support filtering by account ID. \\n\\nWould you like me to:\\n1. Retrieve **all support cases** (and analyze them for general trends), or  \\n2. Clarify the **specific account ID** you want to analyze (even though the tool cannot filter by it at this time)?\\n\\nLet me know how you\\'d like to proceed!<|im_end|>\\n<|im_start|>user\\nCreate a pdf with the content \\'hello world\\'<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 05-04 03:46:27 [async_llm.py:252] Added request chatcmpl-8a75f67bc7964736bd972623edc43051.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:41402 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-04 03:46:34 [loggers.py:111] Engine 000: Avg prompt throughput: 88.3 tokens/s, Avg generation throughput: 31.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 62.4%\n",
      "INFO 05-04 03:46:44 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 62.4%\n",
      "INFO 05-04 03:46:54 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 62.4%\n",
      "INFO 05-04 07:04:18 [logger.py:39] Received request chatcmpl-82cfd85915454ed5aaf6166fe8a1df3c: prompt: '<|im_start|>system\\nYou are a helpful assistant\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_add_reaction\", \"description\": \"Add a reaction emoji to a message\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel containing the message\"}, \"timestamp\": {\"type\": \"string\", \"description\": \"The timestamp of the message to react to\"}, \"reaction\": {\"type\": \"string\", \"description\": \"The name of the emoji reaction (without ::)\"}}, \"required\": [\"channel_id\", \"timestamp\", \"reaction\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_channel_history\", \"description\": \"Get recent messages from a channel\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel\"}, \"limit\": {\"type\": \"number\", \"description\": \"Number of messages to retrieve (default 10)\"}}, \"required\": [\"channel_id\", \"limit\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_thread_replies\", \"description\": \"Get all replies in a message thread\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel containing the thread\"}, \"thread_ts\": {\"type\": \"string\", \"description\": \"The timestamp of the parent message in the format \\'1234567890.123456\\'. Timestamps in the format without the period can be converted by adding the period such that 6 numbers come after it.\"}}, \"required\": [\"channel_id\", \"thread_ts\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_user_profile\", \"description\": \"Get detailed profile information for a specific user\", \"parameters\": {\"type\": \"object\", \"properties\": {\"user_id\": {\"type\": \"string\", \"description\": \"The ID of the user\"}}, \"required\": [\"user_id\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_users\", \"description\": \"Get a list of all users in the workspace with their basic profile information\", \"parameters\": {\"type\": \"object\", \"properties\": {\"cursor\": {\"type\": \"string\", \"description\": \"Pagination cursor for next page of results\"}, \"limit\": {\"type\": \"number\", \"description\": \"Maximum number of users to return (default 100, max 200)\"}}, \"required\": [\"cursor\", \"limit\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_list_channels\", \"description\": \"List public or pre-defined channels in the workspace with pagination\", \"parameters\": {\"type\": \"object\", \"properties\": {\"limit\": {\"type\": \"number\", \"description\": \"Maximum number of channels to return (default 100, max 200)\"}, \"cursor\": {\"type\": \"string\", \"description\": \"Pagination cursor for next page of results\"}}, \"required\": [\"limit\", \"cursor\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_post_message\", \"description\": \"Post a new message to a Slack channel\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel to post to\"}, \"text\": {\"type\": \"string\", \"description\": \"The message text to post\"}}, \"required\": [\"channel_id\", \"text\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_reply_to_thread\", \"description\": \"Reply to a specific message thread in Slack\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel containing the thread\"}, \"thread_ts\": {\"type\": \"string\", \"description\": \"The timestamp of the parent message in the format \\'1234567890.123456\\'. Timestamps in the format without the period can be converted by adding the period such that 6 numbers come after it.\"}, \"text\": {\"type\": \"string\", \"description\": \"The reply text\"}}, \"required\": [\"channel_id\", \"thread_ts\", \"text\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getOpportunities\", \"description\": \"Get a list of active opportunities from the CRM system\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getSupportCases\", \"description\": \"Get a list of support cases\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nReview the current opportunities for ACME<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.9, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 05-04 07:04:18 [async_llm.py:252] Added request chatcmpl-82cfd85915454ed5aaf6166fe8a1df3c.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:39604 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-04 07:04:23 [logger.py:39] Received request chatcmpl-1fced032b61f457c96fe28bbce5544e5: prompt: '<|im_start|>system\\nYou are a helpful assistant\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_add_reaction\", \"description\": \"Add a reaction emoji to a message\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel containing the message\"}, \"timestamp\": {\"type\": \"string\", \"description\": \"The timestamp of the message to react to\"}, \"reaction\": {\"type\": \"string\", \"description\": \"The name of the emoji reaction (without ::)\"}}, \"required\": [\"channel_id\", \"timestamp\", \"reaction\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_channel_history\", \"description\": \"Get recent messages from a channel\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel\"}, \"limit\": {\"type\": \"number\", \"description\": \"Number of messages to retrieve (default 10)\"}}, \"required\": [\"channel_id\", \"limit\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_thread_replies\", \"description\": \"Get all replies in a message thread\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel containing the thread\"}, \"thread_ts\": {\"type\": \"string\", \"description\": \"The timestamp of the parent message in the format \\'1234567890.123456\\'. Timestamps in the format without the period can be converted by adding the period such that 6 numbers come after it.\"}}, \"required\": [\"channel_id\", \"thread_ts\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_user_profile\", \"description\": \"Get detailed profile information for a specific user\", \"parameters\": {\"type\": \"object\", \"properties\": {\"user_id\": {\"type\": \"string\", \"description\": \"The ID of the user\"}}, \"required\": [\"user_id\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_users\", \"description\": \"Get a list of all users in the workspace with their basic profile information\", \"parameters\": {\"type\": \"object\", \"properties\": {\"cursor\": {\"type\": \"string\", \"description\": \"Pagination cursor for next page of results\"}, \"limit\": {\"type\": \"number\", \"description\": \"Maximum number of users to return (default 100, max 200)\"}}, \"required\": [\"cursor\", \"limit\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_list_channels\", \"description\": \"List public or pre-defined channels in the workspace with pagination\", \"parameters\": {\"type\": \"object\", \"properties\": {\"limit\": {\"type\": \"number\", \"description\": \"Maximum number of channels to return (default 100, max 200)\"}, \"cursor\": {\"type\": \"string\", \"description\": \"Pagination cursor for next page of results\"}}, \"required\": [\"limit\", \"cursor\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_post_message\", \"description\": \"Post a new message to a Slack channel\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel to post to\"}, \"text\": {\"type\": \"string\", \"description\": \"The message text to post\"}}, \"required\": [\"channel_id\", \"text\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_reply_to_thread\", \"description\": \"Reply to a specific message thread in Slack\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel containing the thread\"}, \"thread_ts\": {\"type\": \"string\", \"description\": \"The timestamp of the parent message in the format \\'1234567890.123456\\'. Timestamps in the format without the period can be converted by adding the period such that 6 numbers come after it.\"}, \"text\": {\"type\": \"string\", \"description\": \"The reply text\"}}, \"required\": [\"channel_id\", \"thread_ts\", \"text\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getOpportunities\", \"description\": \"Get a list of active opportunities from the CRM system\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getSupportCases\", \"description\": \"Get a list of support cases\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nReview the current opportunities for ACME<|im_end|>\\n<|im_start|>assistant\\n<tool_call>\\n{\"name\": \"getOpportunities\", \"arguments\": {}}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nPost a slack message in the agentic-ai-slack channel with the opportunities<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.9, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 05-04 07:04:23 [async_llm.py:252] Added request chatcmpl-1fced032b61f457c96fe28bbce5544e5.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:39604 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 05-04 07:04:25 [loggers.py:111] Engine 000: Avg prompt throughput: 231.3 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 57.8%\n",
      "INFO 05-04 07:04:35 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 57.8%\n",
      "INFO 05-04 07:04:45 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 57.8%\n",
      "INFO 05-04 07:04:55 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 57.8%\n",
      "INFO 05-04 07:05:05 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 57.8%\n",
      "INFO 05-04 07:05:15 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 57.8%\n",
      "INFO 05-04 07:05:25 [loggers.py:111] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 30.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 57.8%\n",
      "INFO 05-04 07:05:29 [logger.py:39] Received request chatcmpl-54a82f8eb53f4a24939fc4bf7e3ad917: prompt: '<|im_start|>system\\nYou are a helpful assistant\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_add_reaction\", \"description\": \"Add a reaction emoji to a message\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel containing the message\"}, \"timestamp\": {\"type\": \"string\", \"description\": \"The timestamp of the message to react to\"}, \"reaction\": {\"type\": \"string\", \"description\": \"The name of the emoji reaction (without ::)\"}}, \"required\": [\"channel_id\", \"timestamp\", \"reaction\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_channel_history\", \"description\": \"Get recent messages from a channel\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel\"}, \"limit\": {\"type\": \"number\", \"description\": \"Number of messages to retrieve (default 10)\"}}, \"required\": [\"channel_id\", \"limit\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_thread_replies\", \"description\": \"Get all replies in a message thread\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel containing the thread\"}, \"thread_ts\": {\"type\": \"string\", \"description\": \"The timestamp of the parent message in the format \\'1234567890.123456\\'. Timestamps in the format without the period can be converted by adding the period such that 6 numbers come after it.\"}}, \"required\": [\"channel_id\", \"thread_ts\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_user_profile\", \"description\": \"Get detailed profile information for a specific user\", \"parameters\": {\"type\": \"object\", \"properties\": {\"user_id\": {\"type\": \"string\", \"description\": \"The ID of the user\"}}, \"required\": [\"user_id\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_get_users\", \"description\": \"Get a list of all users in the workspace with their basic profile information\", \"parameters\": {\"type\": \"object\", \"properties\": {\"cursor\": {\"type\": \"string\", \"description\": \"Pagination cursor for next page of results\"}, \"limit\": {\"type\": \"number\", \"description\": \"Maximum number of users to return (default 100, max 200)\"}}, \"required\": [\"cursor\", \"limit\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_list_channels\", \"description\": \"List public or pre-defined channels in the workspace with pagination\", \"parameters\": {\"type\": \"object\", \"properties\": {\"limit\": {\"type\": \"number\", \"description\": \"Maximum number of channels to return (default 100, max 200)\"}, \"cursor\": {\"type\": \"string\", \"description\": \"Pagination cursor for next page of results\"}}, \"required\": [\"limit\", \"cursor\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_post_message\", \"description\": \"Post a new message to a Slack channel\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel to post to\"}, \"text\": {\"type\": \"string\", \"description\": \"The message text to post\"}}, \"required\": [\"channel_id\", \"text\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"slack_reply_to_thread\", \"description\": \"Reply to a specific message thread in Slack\", \"parameters\": {\"type\": \"object\", \"properties\": {\"channel_id\": {\"type\": \"string\", \"description\": \"The ID of the channel containing the thread\"}, \"thread_ts\": {\"type\": \"string\", \"description\": \"The timestamp of the parent message in the format \\'1234567890.123456\\'. Timestamps in the format without the period can be converted by adding the period such that 6 numbers come after it.\"}, \"text\": {\"type\": \"string\", \"description\": \"The reply text\"}}, \"required\": [\"channel_id\", \"thread_ts\", \"text\"]}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getOpportunities\", \"description\": \"Get a list of active opportunities from the CRM system\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n{\"type\": \"function\", \"function\": {\"name\": \"getSupportCases\", \"description\": \"Get a list of support cases\", \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}}}\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nReview the current opportunities for ACME<|im_end|>\\n<|im_start|>assistant\\n<tool_call>\\n{\"name\": \"getOpportunities\", \"arguments\": {}}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nPost a slack message in the agentic-ai-slack channel with the opportunities<|im_end|>\\n<|im_start|>assistant\\n<tool_call>\\n{\"name\": \"getOpportunities\", \"arguments\": {}}\\n</tool_call><|im_end|>\\n<|im_start|>user\\nGet a list of support cases for the account<|im_end|>\\n<|im_start|>assistant\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.9, top_k=20, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 05-04 07:05:29 [async_llm.py:252] Added request chatcmpl-54a82f8eb53f4a24939fc4bf7e3ad917.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:39604 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1,2,3 vllm serve \"Qwen/Qwen3-14B\" \\\n",
    "    --port 8989 \\\n",
    "    --api-key alanliuxiang \\\n",
    "    --tensor-parallel-size 4 \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser granite \\\n",
    "    --gpu-memory-utilization 0.8 \\\n",
    "    --max_model_len 34688"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
