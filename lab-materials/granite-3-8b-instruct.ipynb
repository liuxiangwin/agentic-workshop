{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711467ab-4bda-4576-8bac-ed570ca5fa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/opt/app-root/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/opt/app-root/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogen-core 0.4.2 requires pillow>=11.0.0, but you have pillow 10.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/opt/app-root/lib64/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vllm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3a9a8ab-3ebb-49e6-80ff-58268d385bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !vllm serve \"ibm-granite/granite-3.0-8b-instruct\" --dtype auto --api-key alanliuxiang\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca56ffb-e012-4fea-b90b-1c6e9c92d81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-17 10:28:34 api_server.py:712] vLLM API server version 0.6.6.post1\n",
      "INFO 01-17 10:28:34 api_server.py:713] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.0-8b-instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='alanliuxiang', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=True, tool_call_parser='granite', tool_parser_plugin='', model='ibm-granite/granite-3.0-8b-instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f900776a8e0>)\n",
      "INFO 01-17 10:28:34 api_server.py:199] Started engine process with PID 6708\n",
      "INFO 01-17 10:28:42 config.py:510] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 01-17 10:28:47 config.py:510] This model supports multiple tasks: {'reward', 'score', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-17 10:28:47 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='ibm-granite/granite-3.0-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.0-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ibm-granite/granite-3.0-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n",
      "INFO 01-17 10:28:50 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-17 10:28:52 model_runner.py:1094] Starting to load model ibm-granite/granite-3.0-8b-instruct...\n",
      "INFO 01-17 10:28:52 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.28s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.31s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.10it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.06s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.08s/it]\n",
      "\n",
      "INFO 01-17 10:28:57 model_runner.py:1099] Loading model weights took 15.2208 GB\n",
      "INFO 01-17 10:28:58 worker.py:241] Memory profiling takes 0.85 seconds\n",
      "INFO 01-17 10:28:58 worker.py:241] the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB\n",
      "INFO 01-17 10:28:58 worker.py:241] model weights take 15.22GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.48GiB; the rest of the memory reserved for KV Cache is 19.63GiB.\n",
      "INFO 01-17 10:28:58 gpu_executor.py:76] # GPU blocks: 8041, # CPU blocks: 1638\n",
      "INFO 01-17 10:28:58 gpu_executor.py:80] Maximum concurrency for 4096 tokens per request: 31.41x\n",
      "INFO 01-17 10:29:00 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:16<00:00,  2.14it/s]\n",
      "INFO 01-17 10:29:16 model_runner.py:1535] Graph capturing finished in 16 secs, took 0.31 GiB\n",
      "INFO 01-17 10:29:16 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 19.58 seconds\n",
      "INFO 01-17 10:29:17 api_server.py:640] Using supplied chat template:\n",
      "INFO 01-17 10:29:17 api_server.py:640] None\n",
      "INFO 01-17 10:29:17 serving_chat.py:73] \"auto\" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.\n",
      "INFO 01-17 10:29:17 launcher.py:19] Available routes are:\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /pooling, Methods: POST\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /score, Methods: POST\n",
      "INFO 01-17 10:29:17 launcher.py:27] Route: /v1/score, Methods: POST\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m6574\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 01-17 10:31:46 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "INFO 01-17 10:31:46 logger.py:37] Received request chatcmpl-b3955bcf67384e208cbafe2d481c773b: prompt: '<|start_of_role|>available_tools<|end_of_role|>\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"duckduckgo_search\",\\n        \"description\": \"A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\",\\n        \"parameters\": {\\n            \"properties\": {\\n                \"query\": {\\n                    \"description\": \"search query to look up\",\\n                    \"type\": \"string\"\\n                }\\n            },\\n            \"required\": [\\n                \"query\"\\n            ],\\n            \"type\": \"object\"\\n        }\\n    }\\n}\\n\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"python_repl\",\\n        \"description\": \"Use this to execute python code. If you want to see the output of a value,\\\\n    you should print it out with `print(...)`. This is visible to the user.\",\\n        \"parameters\": {\\n            \"properties\": {\\n                \"code\": {\\n                    \"description\": \"The python code to execute to generate your calculations.\",\\n                    \"type\": \"string\"\\n                }\\n            },\\n            \"required\": [\\n                \"code\"\\n            ],\\n            \"type\": \"object\"\\n        }\\n    }\\n}<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What is the biggest mountain in the world?<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:52210 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-17 10:31:46 engine.py:267] Added request chatcmpl-b3955bcf67384e208cbafe2d481c773b.\n",
      "INFO 01-17 10:31:46 metrics.py:467] Avg prompt throughput: 29.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-17 10:31:47 logger.py:37] Received request chatcmpl-10af00e137654f6f83054f4d749374da: prompt: '<|start_of_role|>available_tools<|end_of_role|>\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"duckduckgo_search\",\\n        \"description\": \"A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\",\\n        \"parameters\": {\\n            \"properties\": {\\n                \"query\": {\\n                    \"description\": \"search query to look up\",\\n                    \"type\": \"string\"\\n                }\\n            },\\n            \"required\": [\\n                \"query\"\\n            ],\\n            \"type\": \"object\"\\n        }\\n    }\\n}\\n\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"python_repl\",\\n        \"description\": \"Use this to execute python code. If you want to see the output of a value,\\\\n    you should print it out with `print(...)`. This is visible to the user.\",\\n        \"parameters\": {\\n            \"properties\": {\\n                \"code\": {\\n                    \"description\": \"The python code to execute to generate your calculations.\",\\n                    \"type\": \"string\"\\n                }\\n            },\\n            \"required\": [\\n                \"code\"\\n            ],\\n            \"type\": \"object\"\\n        }\\n    }\\n}<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What is the biggest mountain in the world?<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>The biggest mountain in the world is Mount Everest, with a height of approximately 8,848.86 meters (29,031.7 feet) above sea level.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>Obtain the area of the mountain<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:52210 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-17 10:31:47 engine.py:267] Added request chatcmpl-10af00e137654f6f83054f4d749374da.\n",
      "INFO 01-17 10:31:49 logger.py:37] Received request chatcmpl-93c20cbe30a844ce8d4c2dcacc43ef94: prompt: '<|start_of_role|>available_tools<|end_of_role|>\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"duckduckgo_search\",\\n        \"description\": \"A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\",\\n        \"parameters\": {\\n            \"properties\": {\\n                \"query\": {\\n                    \"description\": \"search query to look up\",\\n                    \"type\": \"string\"\\n                }\\n            },\\n            \"required\": [\\n                \"query\"\\n            ],\\n            \"type\": \"object\"\\n        }\\n    }\\n}\\n\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"python_repl\",\\n        \"description\": \"Use this to execute python code. If you want to see the output of a value,\\\\n    you should print it out with `print(...)`. This is visible to the user.\",\\n        \"parameters\": {\\n            \"properties\": {\\n                \"code\": {\\n                    \"description\": \"The python code to execute to generate your calculations.\",\\n                    \"type\": \"string\"\\n                }\\n            },\\n            \"required\": [\\n                \"code\"\\n            ],\\n            \"type\": \"object\"\\n        }\\n    }\\n}<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What is the biggest mountain in the world?<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>The biggest mountain in the world is Mount Everest, with a height of approximately 8,848.86 meters (29,031.7 feet) above sea level.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>Obtain the area of the mountain<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>I\\'m sorry for the confusion, but mountains don\\'t have an area in the same way that flat surfaces do. The term \"area\" is typically used to describe the size of a two-dimensional surface. Mountains are three-dimensional objects, and their size is usually described using terms like height, volume, or base circumference.\\n\\nIf you\\'re interested in the base circumference of Mount Everest, it\\'s approximately 26,760 meters (16,624 miles). However, this doesn\\'t represent the \"area\" of the mountain.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>Now calculate the distance between the nearest city of the mountain. Select the closest<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:52210 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-17 10:31:49 engine.py:267] Added request chatcmpl-93c20cbe30a844ce8d4c2dcacc43ef94.\n",
      "INFO 01-17 10:31:51 logger.py:37] Received request chatcmpl-146f52e155b8447f8c4e7ecb5867fc5a: prompt: '<|start_of_role|>available_tools<|end_of_role|>\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"duckduckgo_search\",\\n        \"description\": \"A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\",\\n        \"parameters\": {\\n            \"properties\": {\\n                \"query\": {\\n                    \"description\": \"search query to look up\",\\n                    \"type\": \"string\"\\n                }\\n            },\\n            \"required\": [\\n                \"query\"\\n            ],\\n            \"type\": \"object\"\\n        }\\n    }\\n}\\n\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"python_repl\",\\n        \"description\": \"Use this to execute python code. If you want to see the output of a value,\\\\n    you should print it out with `print(...)`. This is visible to the user.\",\\n        \"parameters\": {\\n            \"properties\": {\\n                \"code\": {\\n                    \"description\": \"The python code to execute to generate your calculations.\",\\n                    \"type\": \"string\"\\n                }\\n            },\\n            \"required\": [\\n                \"code\"\\n            ],\\n            \"type\": \"object\"\\n        }\\n    }\\n}<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What is the biggest mountain in the world?<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>The biggest mountain in the world is Mount Everest, with a height of approximately 8,848.86 meters (29,031.7 feet) above sea level.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>Obtain the area of the mountain<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>I\\'m sorry for the confusion, but mountains don\\'t have an area in the same way that flat surfaces do. The term \"area\" is typically used to describe the size of a two-dimensional surface. Mountains are three-dimensional objects, and their size is usually described using terms like height, volume, or base circumference.\\n\\nIf you\\'re interested in the base circumference of Mount Everest, it\\'s approximately 26,760 meters (16,624 miles). However, this doesn\\'t represent the \"area\" of the mountain.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>Now calculate the distance between the nearest city of the mountain. Select the closest<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>The nearest city to Mount Everest is Lukla, Nepal. The distance between Lukla and the base of Mount Everest is approximately 5,000 meters (16,404 feet) as the crow flies. However, due to the mountainous terrain, the actual distance by road or trail is much greater.\\n\\nIf you\\'re interested in the distance to other cities, please provide the specific city names, and I\\'ll do my best to provide the information.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>\\nWhat is the biggest mountain in the world?\\nObtain the area of the mountain\\nNow calculate the distance between the nearest city of the mountain. Select the closest\\nCan I climb it with Flip Flops? Which shoes do recommend me to wear?\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:52210 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-17 10:31:51 engine.py:267] Added request chatcmpl-146f52e155b8447f8c4e7ecb5867fc5a.\n",
      "INFO 01-17 10:31:51 metrics.py:467] Avg prompt throughput: 295.8 tokens/s, Avg generation throughput: 58.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-17 10:32:06 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-17 10:32:16 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "ERROR 01-17 12:48:14 serving_chat.py:114] Error with model object='error' message='The model `mistralai/Mistral-7B-Instruct-v0.3` does not exist.' type='NotFoundError' param=None code=404\n",
      "\u001b[32mINFO\u001b[0m:     ::1:60074 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "INFO 01-17 13:13:47 logger.py:37] Received request chatcmpl-fdb78d0abb024cdb9a195b74318c2a57: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe.\\nYou will be asked a question, to which you must give an answer.\\nYour answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\\nPlease ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect.\\nIf the question requires real-time information or specific technical analysis that you cannot provide directly, respond by indicating you will use a tool to retrieve the information and then use the tool to provide the accurate data.\\n<</SYS>>\\n\\n### QUESTION:\\nanalyze why dolomites is better than catalan pyrenees for hiking\\n\\n### ANSWER:\\nIf this question requires real-time information or analysis, I will retrieve the data for you using a reliable source or tool.\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:40480 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-17 13:13:47 engine.py:267] Added request chatcmpl-fdb78d0abb024cdb9a195b74318c2a57.\n",
      "INFO 01-17 13:13:51 metrics.py:467] Avg prompt throughput: 48.1 tokens/s, Avg generation throughput: 44.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-17 13:13:52 logger.py:37] Received request chatcmpl-f5280544e9894db39aac5e0b34355175: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe.\\nYou will be asked a question, to which you must give an answer.\\nYour answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\\nPlease ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect.\\nIf the question requires real-time information or specific technical analysis that you cannot provide directly, respond by indicating you will use a tool to retrieve the information and then use the tool to provide the accurate data.\\n<</SYS>>\\n\\n### QUESTION:\\nwhat gear should i bring to hike mountains like dolomites?\\n\\n### ANSWER:\\nIf this question requires real-time information or analysis, I will retrieve the data for you using a reliable source or tool.\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:40480 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-17 13:13:52 engine.py:267] Added request chatcmpl-f5280544e9894db39aac5e0b34355175.\n",
      "INFO 01-17 13:13:56 metrics.py:467] Avg prompt throughput: 47.7 tokens/s, Avg generation throughput: 60.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-17 13:14:07 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-17 13:14:17 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-17 13:21:20 logger.py:37] Received request chatcmpl-745bf4ac6a4e4723946c29cbaa0281c7: prompt: '<|start_of_role|>system<|end_of_role|>You are a helpful AI Assistant.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What is Agentic AI.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4070, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 01-17 13:21:20 engine.py:267] Added request chatcmpl-745bf4ac6a4e4723946c29cbaa0281c7.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:57978 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:57988 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58004 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58012 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58022 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58026 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58032 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58042 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58048 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58058 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58068 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58084 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58098 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58102 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58110 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58122 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58132 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58148 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181] Error in preprocessing prompt inputs\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181] Traceback (most recent call last):\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 165, in create_chat_completion\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]     ) = await self._preprocess_chat(\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 502, in _preprocess_chat\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]     prompt_inputs = await self._tokenize_prompt_input_async(\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]   File \"/usr/lib64/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]     result = self.fn(*self.args, **self.kwargs)\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 336, in _tokenize_prompt_input\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]     return next(\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]            ^^^^^\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 359, in _tokenize_prompt_inputs\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]     yield self._normalize_prompt_text_to_input(\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 255, in _normalize_prompt_text_to_input\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]     return self._validate_input(request, input_ids, input_text)\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 308, in _validate_input\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181]     raise ValueError(\n",
      "ERROR 01-17 13:21:24 serving_chat.py:181] ValueError: This model's maximum context length is 4096 tokens. However, you requested 4133 tokens in the messages, Please reduce the length of the messages.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:58156 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[31m400 Bad Request\u001b[0m\n",
      "INFO 01-17 13:21:31 metrics.py:467] Avg prompt throughput: 1.9 tokens/s, Avg generation throughput: 5.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-17 13:21:41 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-17 13:25:20 logger.py:37] Received request chatcmpl-f537a4c491f842feb5de6520e2c560e6: prompt: '<|start_of_role|>system<|end_of_role|>You are a helpful AI Assistant.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What is Agentic AI.<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4070, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "INFO 01-17 13:25:20 engine.py:267] Added request chatcmpl-f537a4c491f842feb5de6520e2c560e6.\n",
      "INFO 01-17 13:25:20 metrics.py:467] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35584 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35600 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35614 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35624 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35634 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35648 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35656 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35670 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35682 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35690 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35694 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35702 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35706 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35708 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35718 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35734 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35744 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35760 - \"\u001b[1mPOST /currency_calculator HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181] Error in preprocessing prompt inputs\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181] Traceback (most recent call last):\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 165, in create_chat_completion\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]     ) = await self._preprocess_chat(\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 502, in _preprocess_chat\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]     prompt_inputs = await self._tokenize_prompt_input_async(\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]   File \"/usr/lib64/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]     result = self.fn(*self.args, **self.kwargs)\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 336, in _tokenize_prompt_input\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]     return next(\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]            ^^^^^\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 359, in _tokenize_prompt_inputs\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]     yield self._normalize_prompt_text_to_input(\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 255, in _normalize_prompt_text_to_input\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]     return self._validate_input(request, input_ids, input_text)\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]   File \"/opt/app-root/lib64/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py\", line 308, in _validate_input\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181]     raise ValueError(\n",
      "ERROR 01-17 13:25:24 serving_chat.py:181] ValueError: This model's maximum context length is 4096 tokens. However, you requested 4133 tokens in the messages, Please reduce the length of the messages.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35764 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[31m400 Bad Request\u001b[0m\n",
      "INFO 01-17 13:25:31 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-17 13:25:41 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "!vllm serve \"ibm-granite/granite-3.0-8b-instruct\" \\\n",
    "    --api-key alanliuxiang \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser granite "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
