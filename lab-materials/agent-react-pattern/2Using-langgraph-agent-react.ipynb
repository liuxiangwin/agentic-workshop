{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eaa112b-020e-4b78-8bcf-f72dab6d0e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install tavily-python \n",
    "!pip install -U tavily-python langchain_community --quiet\n",
    "!pip install autogen-agentchat~=0.2 autogen psutil --quiet\n",
    "# !pip install -q langchain-openai termcolor langchain_community duckduckgo_search wikipedia openapi-python-client==0.12.3 langgraph langchain_experimental yfinance\n",
    "!pip install -q langchain-openai langchain-anthropic termcolor langchain_community duckduckgo_search wikipedia openapi-python-client langgraph langchain_experimental openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a12ded-0eee-403e-970c-1497de0d064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd8229b-5e18-486a-8548-d88bf4383dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0054110-eb64-44f8-ae3d-326da00446ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import autogen\n",
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "import autogen\n",
    "from autogen.cache import Cache\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "INFERENCE_SERVER_URL = \"http://localhost:8000\"\n",
    "MODEL_NAME = \"ibm-granite/granite-3.0-8b-instruct\"\n",
    "API_KEY= \"alanliuxiang\"\n",
    "\n",
    "# INFERENCE_SERVER_URL = \"https://api.feidaapi.com/\"\n",
    "# MODEL_NAME = \"gpt-4o\"\n",
    "# API_KEY= \"sk-TsoMJYTiW1Ya1AatboGdUGXEdCOAWeCeIbvUCkNMonrCHoQ7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda94037-35fd-4f67-95ef-3d5942dbe65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from langgraph.types import Command, interrupt\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "import operator\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base= f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "# tools\n",
    "tool = TavilySearchResults(max_results=4)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    \n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        \n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        \n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "    \n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "    \n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], \n",
    "                                       name=t['name'], \n",
    "                                       content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}\n",
    "    \n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f007afb-3845-42d6-bca7-5c6905875a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry for the confusion, but as of now, the ICC T20 World Cup in 2024 has not yet taken place. Therefore, I cannot provide the name of the winning country or its capital city. Please check back after the event for accurate information.I'm sorry for the confusion, but as of now, the ICC T20 World Cup in 2024 has not yet taken place. Therefore, I cannot provide the name of the winning country or its capital city. Please check back after the event for accurate information.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\"\"\"\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4-turbo\") #reduce inference cost\n",
    "abot = Agent(model, [tool], system=prompt)\n",
    "\n",
    "question = \"\"\"Which country won the ICC t20  World Cup in 2024? \n",
    "What is the name of the capital city of that county?\"\"\"\n",
    "\n",
    "messages = [HumanMessage(content=question)]\n",
    "result = abot.graph.invoke({'messages': messages})\n",
    "print(result['messages'][-1].content)\n",
    "\n",
    "# \"\"\"\n",
    "# Calling: {'name': 'tavily_search_results_json', \n",
    "# 'args': {'query': 'ICC T20 World Cup 2024 winner'}, 'id': 'call_3X2XeLNJ3RSHZVxYgJyNGLlI', 'type': 'tool_call'}\n",
    "\n",
    "# Back to the model!\n",
    "# Calling: {'name': 'tavily_search_results_json', \n",
    "# 'args': {'query': 'capital city of India'}, 'id': 'call_vkgaIdrtoIecRVpODo7WyatY', 'type': 'tool_call'}\n",
    "\n",
    "# Back to the model!\n",
    "# India won the ICC T20 World Cup in 2024. \n",
    "# The capital city of India is New Delhi.\n",
    "# \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
