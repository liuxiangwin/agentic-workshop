{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "711467ab-4bda-4576-8bac-ed570ca5fa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vllm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a9a8ab-3ebb-49e6-80ff-58268d385bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-16 12:24:07 api_server.py:712] vLLM API server version 0.6.6.post1\n",
      "INFO 01-16 12:24:07 api_server.py:713] args: Namespace(subparser='serve', model_tag='ibm-granite/granite-3.0-8b-instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='alanliuxiang', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='ibm-granite/granite-3.0-8b-instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f65eeb65940>)\n",
      "INFO 01-16 12:24:07 api_server.py:199] Started engine process with PID 1000\n",
      "config.json: 100%|█████████████████████████████| 788/788 [00:00<00:00, 6.69MB/s]\n",
      "INFO 01-16 12:24:13 config.py:510] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "tokenizer_config.json: 100%|███████████████| 5.64k/5.64k [00:00<00:00, 41.2MB/s]\n",
      "vocab.json: 100%|████████████████████████████| 777k/777k [00:00<00:00, 16.0MB/s]\n",
      "merges.txt: 100%|████████████████████████████| 442k/442k [00:00<00:00, 21.6MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 3.48M/3.48M [00:00<00:00, 14.4MB/s]\n",
      "added_tokens.json: 100%|█████████████████████| 87.0/87.0 [00:00<00:00, 1.46MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 701/701 [00:00<00:00, 4.25MB/s]\n",
      "INFO 01-16 12:24:17 config.py:510] This model supports multiple tasks: {'generate', 'score', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 01-16 12:24:17 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='ibm-granite/granite-3.0-8b-instruct', speculative_config=None, tokenizer='ibm-granite/granite-3.0-8b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=ibm-granite/granite-3.0-8b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=True, \n",
      "generation_config.json: 100%|███████████████████| 137/137 [00:00<00:00, 589kB/s]\n",
      "INFO 01-16 12:24:17 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-16 12:24:18 model_runner.py:1094] Starting to load model ibm-granite/granite-3.0-8b-instruct...\n",
      "INFO 01-16 12:24:18 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "model-00001-of-00004.safetensors:   0%|             | 0.00/4.97G [00:00<?, ?B/s]\n",
      "model-00002-of-00004.safetensors:   0%|             | 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   0%|             | 0.00/4.97G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0%|    | 10.5M/4.97G [00:00<01:52, 44.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   0%|    | 10.5M/4.99G [00:00<01:56, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   0%|    | 10.5M/4.97G [00:00<01:56, 42.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0%|    | 21.0M/4.97G [00:00<01:54, 43.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   0%|    | 21.0M/4.97G [00:00<01:55, 42.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   0%|    | 21.0M/4.99G [00:00<01:57, 42.5MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   1%|    | 31.5M/4.97G [00:00<01:55, 42.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   1%|    | 31.5M/4.97G [00:00<01:54, 43.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 41.9M/4.97G [00:00<01:54, 43.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   1%|    | 41.9M/4.97G [00:00<01:55, 42.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   2%|    | 31.5M/1.41G [00:00<00:45, 30.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 52.4M/4.97G [00:01<01:54, 43.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   1%|    | 52.4M/4.97G [00:01<01:54, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   3%|    | 41.9M/1.41G [00:01<00:39, 34.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 62.9M/4.97G [00:01<01:54, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   1%|    | 62.9M/4.97G [00:01<01:54, 43.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   4%|▏   | 52.4M/1.41G [00:01<00:37, 36.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 73.4M/4.97G [00:01<01:54, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   1%|    | 73.4M/4.97G [00:01<01:54, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   4%|▏   | 62.9M/1.41G [00:01<00:35, 38.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|    | 83.9M/4.97G [00:01<01:54, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   2%|    | 83.9M/4.97G [00:01<01:54, 42.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   5%|▏   | 73.4M/1.41G [00:01<00:34, 38.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|    | 94.4M/4.97G [00:02<01:54, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   2%|    | 94.4M/4.97G [00:02<01:53, 42.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   6%|▏   | 83.9M/1.41G [00:02<00:33, 39.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|     | 105M/4.97G [00:02<01:54, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   2%|     | 105M/4.97G [00:02<01:53, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   7%|▎   | 94.4M/1.41G [00:02<00:32, 40.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|     | 115M/4.97G [00:02<01:53, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   2%|     | 115M/4.97G [00:02<01:53, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   7%|▎    | 105M/1.41G [00:02<00:31, 40.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|     | 115M/4.99G [00:02<01:56, 41.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 126M/4.97G [00:02<01:53, 42.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   8%|▍    | 115M/1.41G [00:02<00:31, 41.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 126M/4.99G [00:03<01:56, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 136M/4.97G [00:03<01:54, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   9%|▍    | 126M/1.41G [00:03<00:31, 40.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 136M/4.99G [00:03<01:55, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 147M/4.97G [00:03<01:54, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  10%|▍    | 136M/1.41G [00:03<00:32, 38.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 147M/4.99G [00:03<01:54, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 157M/4.97G [00:03<01:53, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  10%|▌    | 147M/1.41G [00:03<00:32, 39.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 157M/4.99G [00:03<01:54, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 168M/4.97G [00:03<01:52, 42.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  11%|▌    | 157M/1.41G [00:04<00:31, 40.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 178M/4.97G [00:04<01:52, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 178M/4.97G [00:04<01:52, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  12%|▌    | 168M/1.41G [00:04<00:30, 40.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 189M/4.97G [00:04<01:52, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 189M/4.97G [00:04<01:52, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  13%|▋    | 178M/1.41G [00:04<00:29, 41.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 189M/4.99G [00:04<01:52, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 199M/4.97G [00:04<01:52, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  13%|▋    | 189M/1.41G [00:04<00:29, 41.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 199M/4.99G [00:04<01:52, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 210M/4.97G [00:04<01:52, 42.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  14%|▋    | 199M/1.41G [00:05<00:28, 41.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 220M/4.97G [00:05<01:52, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 220M/4.97G [00:05<01:53, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  15%|▋    | 210M/1.41G [00:05<00:28, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▏    | 231M/4.97G [00:05<01:52, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   5%|▏    | 231M/4.97G [00:05<01:52, 42.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  16%|▊    | 220M/1.41G [00:05<00:28, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▏    | 241M/4.97G [00:05<01:51, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   5%|▏    | 241M/4.97G [00:05<01:52, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  16%|▊    | 231M/1.41G [00:05<00:27, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎    | 252M/4.97G [00:05<01:50, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   5%|▎    | 252M/4.97G [00:05<01:51, 42.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  17%|▊    | 241M/1.41G [00:06<00:27, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎    | 262M/4.97G [00:06<01:50, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   5%|▎    | 262M/4.97G [00:06<01:50, 42.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  18%|▉    | 252M/1.41G [00:06<00:27, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎    | 273M/4.97G [00:06<01:50, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   5%|▎    | 273M/4.97G [00:06<01:50, 42.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  19%|▉    | 262M/1.41G [00:06<00:27, 41.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 283M/4.97G [00:06<01:50, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 283M/4.97G [00:06<01:50, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  19%|▉    | 273M/1.41G [00:06<00:26, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 294M/4.97G [00:06<01:50, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 294M/4.97G [00:06<01:50, 42.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  20%|█    | 283M/1.41G [00:07<00:26, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 304M/4.97G [00:07<01:49, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 304M/4.97G [00:07<01:49, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  21%|█    | 294M/1.41G [00:07<00:26, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 315M/4.97G [00:07<01:49, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 315M/4.97G [00:07<01:49, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  22%|█    | 304M/1.41G [00:07<00:25, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 325M/4.97G [00:07<01:48, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 325M/4.97G [00:07<01:49, 42.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  22%|█    | 315M/1.41G [00:07<00:25, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 336M/4.97G [00:07<01:48, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 336M/4.97G [00:07<01:48, 42.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  23%|█▏   | 325M/1.41G [00:08<00:25, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 346M/4.97G [00:08<01:48, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 346M/4.97G [00:08<01:48, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 346M/4.99G [00:08<01:48, 42.8MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 357M/4.97G [00:08<01:48, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 357M/4.97G [00:08<01:48, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 357M/4.99G [00:08<01:48, 42.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 367M/4.97G [00:08<01:47, 42.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 367M/4.97G [00:08<01:48, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 367M/4.99G [00:08<01:48, 42.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 377M/4.97G [00:08<01:47, 42.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 377M/4.97G [00:08<01:47, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 377M/4.99G [00:09<01:48, 42.4MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 388M/4.97G [00:09<01:47, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 388M/4.97G [00:09<01:47, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 388M/4.99G [00:09<01:48, 42.5MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 398M/4.97G [00:09<01:47, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 398M/4.97G [00:09<01:47, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 398M/4.99G [00:09<01:47, 42.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 409M/4.97G [00:09<01:47, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 409M/4.97G [00:09<01:48, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 409M/4.99G [00:09<01:47, 42.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 419M/4.97G [00:09<01:46, 42.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 419M/4.97G [00:09<01:47, 42.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 430M/4.97G [00:10<01:46, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 419M/4.99G [00:10<01:50, 41.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 430M/4.97G [00:10<01:46, 42.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 440M/4.97G [00:10<01:46, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 440M/4.97G [00:10<01:46, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 451M/4.97G [00:10<01:45, 42.7MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  31%|█▌   | 430M/1.41G [00:10<00:23, 42.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 451M/4.97G [00:10<01:45, 42.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 461M/4.97G [00:10<01:45, 42.9MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  31%|█▌   | 440M/1.41G [00:10<00:22, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 461M/4.97G [00:10<01:45, 42.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 472M/4.97G [00:11<01:45, 42.9MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  32%|█▌   | 451M/1.41G [00:10<00:22, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 472M/4.97G [00:11<01:45, 42.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▍    | 482M/4.97G [00:11<01:45, 42.8MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  33%|█▋   | 461M/1.41G [00:11<00:22, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  10%|▍    | 482M/4.97G [00:11<01:45, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▍    | 493M/4.97G [00:11<01:44, 42.7MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  34%|█▋   | 472M/1.41G [00:11<00:21, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  10%|▍    | 493M/4.97G [00:11<01:45, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▍    | 482M/4.99G [00:11<01:51, 40.3MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  10%|▌    | 503M/4.97G [00:11<01:45, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  10%|▌    | 503M/4.97G [00:11<01:44, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▌    | 514M/4.97G [00:12<01:44, 42.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  35%|█▊   | 493M/1.41G [00:11<00:21, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  10%|▌    | 514M/4.97G [00:12<01:44, 42.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 524M/4.97G [00:12<01:44, 42.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  36%|█▊   | 503M/1.41G [00:12<00:21, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 524M/4.97G [00:12<01:44, 42.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 535M/4.97G [00:12<01:43, 42.8MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  37%|█▊   | 514M/1.41G [00:12<00:20, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 535M/4.97G [00:12<01:43, 42.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 545M/4.97G [00:12<01:46, 41.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 545M/4.97G [00:12<01:43, 42.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  37%|█▊   | 524M/1.41G [00:12<00:24, 36.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 556M/4.97G [00:13<01:45, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 556M/4.97G [00:13<01:43, 42.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  38%|█▉   | 535M/1.41G [00:13<00:22, 38.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 545M/4.99G [00:13<01:45, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 566M/4.97G [00:13<01:44, 42.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  39%|█▉   | 545M/1.41G [00:13<00:21, 39.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 556M/4.99G [00:13<01:45, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 577M/4.97G [00:13<01:44, 42.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  40%|█▉   | 556M/1.41G [00:13<00:21, 40.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 566M/4.99G [00:13<01:44, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 587M/4.97G [00:13<01:45, 41.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  40%|██   | 566M/1.41G [00:13<00:20, 41.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 577M/4.99G [00:13<01:44, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 598M/4.97G [00:14<01:43, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  41%|██   | 577M/1.41G [00:14<00:19, 41.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 587M/4.99G [00:14<01:43, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 608M/4.97G [00:14<01:43, 42.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  42%|██   | 587M/1.41G [00:14<00:19, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 619M/4.97G [00:14<01:42, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 619M/4.97G [00:14<01:42, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  43%|██▏  | 598M/1.41G [00:14<00:19, 41.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 629M/4.97G [00:14<01:45, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 629M/4.97G [00:14<01:44, 41.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  43%|██▏  | 608M/1.41G [00:14<00:18, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 640M/4.97G [00:15<01:43, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 640M/4.97G [00:15<01:43, 41.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  44%|██▏  | 619M/1.41G [00:15<00:18, 42.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 650M/4.97G [00:15<01:42, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 650M/4.97G [00:15<01:42, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  45%|██▏  | 629M/1.41G [00:15<00:18, 42.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 661M/4.97G [00:15<01:41, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 661M/4.97G [00:15<01:43, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  46%|██▎  | 640M/1.41G [00:15<00:18, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 671M/4.97G [00:15<01:44, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  14%|▋    | 671M/4.97G [00:15<01:43, 41.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  46%|██▎  | 650M/1.41G [00:15<00:17, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 661M/4.99G [00:15<01:41, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 682M/4.97G [00:16<01:42, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  47%|██▎  | 661M/1.41G [00:16<00:17, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 671M/4.99G [00:16<01:40, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 692M/4.97G [00:16<01:41, 42.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  48%|██▍  | 671M/1.41G [00:16<00:17, 42.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 682M/4.99G [00:16<01:40, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 703M/4.97G [00:16<01:41, 42.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  49%|██▍  | 682M/1.41G [00:16<00:17, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 692M/4.99G [00:16<01:40, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 713M/4.97G [00:16<01:43, 41.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  49%|██▍  | 692M/1.41G [00:16<00:16, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▋    | 724M/4.97G [00:17<01:41, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  15%|▋    | 724M/4.97G [00:17<01:41, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  50%|██▍  | 703M/1.41G [00:17<00:16, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▋    | 734M/4.97G [00:17<01:40, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  15%|▋    | 734M/4.97G [00:17<01:41, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  51%|██▌  | 713M/1.41G [00:17<00:16, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▋    | 744M/4.97G [00:17<01:40, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  15%|▋    | 744M/4.97G [00:17<01:40, 42.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  51%|██▌  | 724M/1.41G [00:17<00:16, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▋    | 734M/4.99G [00:17<01:39, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  15%|▊    | 755M/4.97G [00:17<01:42, 41.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  52%|██▌  | 734M/1.41G [00:17<00:15, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▋    | 744M/4.99G [00:17<01:39, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  15%|▊    | 765M/4.97G [00:18<01:40, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  53%|██▋  | 744M/1.41G [00:18<00:15, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▊    | 755M/4.99G [00:18<01:39, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 776M/4.97G [00:18<01:40, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  54%|██▋  | 755M/1.41G [00:18<00:15, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▊    | 765M/4.99G [00:18<01:39, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 786M/4.97G [00:18<01:39, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  54%|██▋  | 765M/1.41G [00:18<00:15, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 776M/4.99G [00:18<01:39, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 797M/4.97G [00:18<01:40, 41.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  55%|██▊  | 776M/1.41G [00:18<00:14, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 807M/4.97G [00:19<01:38, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 807M/4.97G [00:19<01:39, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  56%|██▊  | 786M/1.41G [00:19<00:14, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 818M/4.97G [00:19<01:39, 41.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 818M/4.97G [00:19<01:39, 41.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  57%|██▊  | 797M/1.41G [00:19<00:14, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 807M/4.99G [00:19<01:38, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 828M/4.97G [00:19<01:38, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  57%|██▊  | 807M/1.41G [00:19<00:14, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 818M/4.99G [00:19<01:38, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 839M/4.97G [00:19<01:39, 41.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  58%|██▉  | 818M/1.41G [00:19<00:13, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 828M/4.99G [00:19<01:37, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 849M/4.97G [00:20<01:37, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  59%|██▉  | 828M/1.41G [00:19<00:13, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 860M/4.97G [00:20<01:36, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 860M/4.97G [00:20<01:37, 42.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  60%|██▉  | 839M/1.41G [00:20<00:13, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 870M/4.97G [00:20<01:36, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 870M/4.97G [00:20<01:36, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  60%|███  | 849M/1.41G [00:20<00:13, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 860M/4.99G [00:20<01:36, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 881M/4.97G [00:20<01:39, 41.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  61%|███  | 860M/1.41G [00:20<00:12, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 870M/4.99G [00:20<01:36, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 891M/4.97G [00:21<01:37, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  62%|███  | 870M/1.41G [00:20<00:12, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 902M/4.97G [00:21<01:36, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 902M/4.97G [00:21<01:36, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  63%|███▏ | 881M/1.41G [00:21<00:12, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 912M/4.97G [00:21<01:36, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 912M/4.97G [00:21<01:36, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  63%|███▏ | 891M/1.41G [00:21<00:12, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 902M/4.99G [00:21<01:38, 41.7MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 923M/4.97G [00:21<01:37, 41.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 923M/4.97G [00:21<01:37, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 912M/4.99G [00:21<01:40, 40.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 933M/4.97G [00:22<01:36, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 933M/4.97G [00:22<01:36, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 923M/4.99G [00:22<01:38, 41.2MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  66%|███▎ | 923M/1.41G [00:22<00:11, 42.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 944M/4.97G [00:22<01:35, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 933M/4.99G [00:22<01:37, 41.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  66%|███▎ | 933M/1.41G [00:22<00:11, 42.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 954M/4.97G [00:22<01:34, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 944M/4.99G [00:22<01:36, 42.1MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  67%|███▎ | 944M/1.41G [00:22<00:10, 42.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 965M/4.97G [00:22<01:37, 41.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 954M/4.99G [00:22<01:35, 42.2MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|▉    | 975M/4.97G [00:23<01:35, 41.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  20%|▉    | 975M/4.97G [00:23<01:35, 41.7MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 965M/4.99G [00:23<01:34, 42.5MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|▉    | 986M/4.97G [00:23<01:34, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  20%|▉    | 986M/4.97G [00:23<01:34, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▉    | 975M/4.99G [00:23<01:34, 42.5MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|█    | 996M/4.97G [00:23<01:34, 42.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  20%|█    | 996M/4.97G [00:23<01:34, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▉    | 986M/4.99G [00:23<01:34, 42.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  70%|███▌ | 986M/1.41G [00:23<00:09, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|▊   | 1.01G/4.97G [00:23<01:35, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▉    | 996M/4.99G [00:23<01:33, 42.7MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20%|▊   | 1.02G/4.97G [00:24<01:34, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▊   | 1.01G/4.99G [00:24<01:33, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  20%|▊   | 1.02G/4.97G [00:24<01:53, 34.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.03G/4.97G [00:24<01:33, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▊   | 1.02G/4.99G [00:24<01:32, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.03G/4.97G [00:24<01:40, 39.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.04G/4.97G [00:24<01:32, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.03G/4.99G [00:24<01:32, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.04G/4.97G [00:24<01:37, 40.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.05G/4.97G [00:24<01:34, 41.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.04G/4.99G [00:24<01:32, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.05G/4.97G [00:24<01:35, 41.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.06G/4.97G [00:25<01:33, 41.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.05G/4.99G [00:25<01:32, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.06G/4.97G [00:25<01:34, 41.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.07G/4.97G [00:25<01:32, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.06G/4.99G [00:25<01:31, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  22%|▊   | 1.07G/4.97G [00:25<01:33, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  22%|▊   | 1.08G/4.97G [00:25<01:32, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.07G/4.99G [00:25<01:31, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  22%|▊   | 1.08G/4.97G [00:25<01:32, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  22%|▉   | 1.09G/4.97G [00:25<01:33, 41.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▊   | 1.08G/4.99G [00:25<01:31, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  22%|▉   | 1.09G/4.97G [00:25<01:31, 42.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  22%|▉   | 1.10G/4.97G [00:26<01:32, 42.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▊   | 1.09G/4.99G [00:26<01:30, 43.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  22%|▉   | 1.10G/4.97G [00:26<01:30, 42.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  22%|▉   | 1.11G/4.97G [00:26<01:31, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▉   | 1.10G/4.99G [00:26<01:30, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  22%|▉   | 1.11G/4.97G [00:26<01:30, 42.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.12G/4.97G [00:26<01:31, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▉   | 1.11G/4.99G [00:26<01:30, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.12G/4.97G [00:26<01:29, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  79%|███▏| 1.11G/1.41G [00:26<00:06, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.13G/4.97G [00:26<01:33, 41.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.13G/4.97G [00:26<01:29, 43.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  80%|███▏| 1.12G/1.41G [00:26<00:06, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.14G/4.97G [00:27<01:31, 41.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.14G/4.97G [00:27<01:29, 42.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  81%|███▏| 1.13G/1.41G [00:27<00:06, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.15G/4.97G [00:27<01:30, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.15G/4.97G [00:27<01:29, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.16G/4.97G [00:27<01:29, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.15G/4.99G [00:27<01:30, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.16G/4.97G [00:27<01:28, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  82%|███▎| 1.15G/1.41G [00:27<00:05, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.17G/4.97G [00:27<01:31, 41.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.17G/4.97G [00:27<01:28, 42.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  83%|███▎| 1.16G/1.41G [00:27<00:05, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.18G/4.97G [00:28<01:30, 41.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.18G/4.97G [00:28<01:28, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  84%|███▎| 1.17G/1.41G [00:28<00:05, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.20G/4.97G [00:28<01:29, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.20G/4.97G [00:28<01:27, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  84%|███▎| 1.18G/1.41G [00:28<00:05, 41.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.21G/4.97G [00:28<01:29, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.21G/4.97G [00:28<01:28, 42.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  85%|███▍| 1.20G/1.41G [00:28<00:05, 41.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.22G/4.97G [00:28<01:30, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.22G/4.97G [00:28<01:28, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  86%|███▍| 1.21G/1.41G [00:28<00:04, 41.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|▉   | 1.23G/4.97G [00:29<01:29, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  25%|▉   | 1.23G/4.97G [00:29<01:28, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  87%|███▍| 1.22G/1.41G [00:29<00:04, 41.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|▉   | 1.24G/4.97G [00:29<01:28, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  25%|▉   | 1.24G/4.97G [00:29<01:27, 42.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  87%|███▍| 1.23G/1.41G [00:29<00:04, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█   | 1.25G/4.97G [00:29<01:27, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  25%|█   | 1.25G/4.97G [00:29<01:27, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  88%|███▌| 1.24G/1.41G [00:29<00:03, 42.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█   | 1.26G/4.97G [00:29<01:29, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  25%|█   | 1.26G/4.97G [00:29<01:27, 42.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  89%|███▌| 1.25G/1.41G [00:29<00:03, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|█   | 1.26G/4.99G [00:29<01:27, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.27G/4.97G [00:30<01:28, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  90%|███▌| 1.26G/1.41G [00:30<00:03, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|█   | 1.27G/4.99G [00:30<01:27, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.28G/4.97G [00:30<01:27, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  90%|███▌| 1.27G/1.41G [00:30<00:03, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.29G/4.97G [00:30<01:26, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.29G/4.97G [00:30<01:27, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  91%|███▋| 1.28G/1.41G [00:30<00:02, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.30G/4.97G [00:30<01:28, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.30G/4.97G [00:30<01:28, 41.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  92%|███▋| 1.29G/1.41G [00:30<00:02, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.31G/4.97G [00:31<01:27, 41.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.31G/4.97G [00:31<01:27, 41.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  93%|███▋| 1.30G/1.41G [00:31<00:02, 42.7MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.32G/4.97G [00:31<01:27, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  27%|█   | 1.32G/4.97G [00:31<01:26, 42.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  93%|███▋| 1.31G/1.41G [00:31<00:02, 42.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.32G/4.99G [00:31<01:26, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.33G/4.97G [00:31<01:27, 41.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  94%|███▊| 1.32G/1.41G [00:31<00:01, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.33G/4.99G [00:31<01:25, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.34G/4.97G [00:31<01:27, 41.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  95%|███▊| 1.33G/1.41G [00:31<00:01, 42.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.35G/4.97G [00:32<01:26, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  96%|███▊| 1.34G/1.41G [00:32<00:01, 42.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.34G/4.99G [00:32<01:53, 32.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.36G/4.97G [00:32<01:25, 42.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  96%|███▊| 1.35G/1.41G [00:32<00:01, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.35G/4.99G [00:32<01:44, 34.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  28%|█   | 1.37G/4.97G [00:32<01:25, 42.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  97%|███▉| 1.36G/1.41G [00:32<00:00, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.36G/4.99G [00:32<01:38, 36.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  28%|█   | 1.38G/4.97G [00:32<01:26, 41.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  98%|███▉| 1.37G/1.41G [00:32<00:00, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█   | 1.39G/4.97G [00:33<01:25, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  28%|█   | 1.39G/4.97G [00:33<01:25, 41.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  99%|███▉| 1.38G/1.41G [00:33<00:00, 42.5MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█▏  | 1.41G/4.97G [00:33<01:24, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  28%|█▏  | 1.41G/4.97G [00:33<01:24, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  99%|███▉| 1.39G/1.41G [00:33<00:00, 42.4MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█▏  | 1.42G/4.97G [00:33<01:23, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  28%|█▏  | 1.42G/4.97G [00:33<01:24, 42.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors: 100%|████| 1.41G/1.41G [00:33<00:00, 41.8MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00004.safetensors:  28%|█▏  | 1.41G/4.99G [00:33<01:27, 40.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.43G/4.97G [00:33<01:25, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█▏  | 1.42G/4.99G [00:33<01:26, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.44G/4.97G [00:34<01:24, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.43G/4.99G [00:34<01:25, 41.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.45G/4.97G [00:34<01:23, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.44G/4.99G [00:34<01:24, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.46G/4.97G [00:34<01:23, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.45G/4.99G [00:34<01:24, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.47G/4.97G [00:34<01:24, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.46G/4.99G [00:34<01:23, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.48G/4.97G [00:35<01:23, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.49G/4.97G [00:35<01:22, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.47G/4.99G [00:35<01:35, 36.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.50G/4.97G [00:35<01:22, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.48G/4.99G [00:35<01:31, 38.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.51G/4.97G [00:35<01:23, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.49G/4.99G [00:35<01:28, 39.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.52G/4.97G [00:36<01:22, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.50G/4.99G [00:36<01:26, 40.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.53G/4.97G [00:36<01:21, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.51G/4.99G [00:36<01:24, 41.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.54G/4.97G [00:36<01:21, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.52G/4.99G [00:36<01:23, 41.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.55G/4.97G [00:36<01:22, 41.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  31%|█▏  | 1.55G/4.97G [00:36<01:22, 41.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▏  | 1.54G/4.99G [00:37<01:22, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  31%|█▎  | 1.56G/4.97G [00:37<01:21, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▏  | 1.55G/4.99G [00:37<01:21, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.57G/4.97G [00:37<01:20, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.58G/4.97G [00:37<01:19, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.58G/4.97G [00:37<01:20, 42.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.57G/4.99G [00:37<01:20, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.59G/4.97G [00:37<01:21, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.60G/4.97G [00:38<01:19, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.60G/4.97G [00:38<01:20, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.61G/4.97G [00:38<01:19, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.61G/4.97G [00:38<01:20, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.63G/4.97G [00:38<01:18, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.63G/4.97G [00:38<01:19, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.61G/4.99G [00:38<01:18, 42.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.64G/4.97G [00:38<01:20, 41.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.63G/4.99G [00:39<01:19, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.65G/4.97G [00:39<01:19, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.64G/4.99G [00:39<01:18, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.66G/4.97G [00:39<01:18, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.67G/4.97G [00:39<01:17, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.67G/4.97G [00:39<01:18, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.68G/4.97G [00:39<01:19, 41.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.68G/4.97G [00:39<01:19, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.69G/4.97G [00:40<01:18, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.69G/4.97G [00:40<01:18, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.70G/4.97G [00:40<01:17, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.70G/4.97G [00:40<01:17, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.71G/4.97G [00:40<01:17, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  34%|█▍  | 1.71G/4.97G [00:40<01:16, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.72G/4.97G [00:40<01:18, 41.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.72G/4.97G [00:40<01:18, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.73G/4.97G [00:41<01:17, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.73G/4.97G [00:41<01:17, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.74G/4.97G [00:41<01:16, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.74G/4.97G [00:41<01:16, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.73G/4.99G [00:41<01:16, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.75G/4.97G [00:41<01:16, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.74G/4.99G [00:41<01:16, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.76G/4.97G [00:41<01:17, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.77G/4.97G [00:42<01:16, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.77G/4.97G [00:42<01:16, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.78G/4.97G [00:42<01:15, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.78G/4.97G [00:42<01:16, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.79G/4.97G [00:42<01:15, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.79G/4.97G [00:42<01:15, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.78G/4.99G [00:42<01:21, 39.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.80G/4.97G [00:42<01:16, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.79G/4.99G [00:43<01:20, 39.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.81G/4.97G [00:43<01:15, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.80G/4.99G [00:43<01:19, 39.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.82G/4.97G [00:43<01:15, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.81G/4.99G [00:43<01:18, 40.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.84G/4.97G [00:43<01:14, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.82G/4.99G [00:43<01:17, 40.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.85G/4.97G [00:43<01:15, 41.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.86G/4.97G [00:44<01:13, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  37%|█▍  | 1.86G/4.97G [00:44<01:14, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.85G/4.99G [00:44<01:15, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.87G/4.97G [00:44<01:13, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.86G/4.99G [00:44<01:14, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.88G/4.97G [00:44<01:13, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.87G/4.99G [00:44<01:13, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.89G/4.97G [00:44<01:14, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.90G/4.97G [00:45<01:13, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.90G/4.97G [00:45<01:13, 41.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.91G/4.97G [00:45<01:12, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.91G/4.97G [00:45<01:12, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.90G/4.99G [00:45<01:12, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.92G/4.97G [00:45<01:12, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.91G/4.99G [00:45<01:12, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.93G/4.97G [00:45<01:13, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.92G/4.99G [00:45<01:12, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.94G/4.97G [00:46<01:12, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.93G/4.99G [00:46<01:11, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.95G/4.97G [00:46<01:11, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.94G/4.99G [00:46<01:11, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.96G/4.97G [00:46<01:11, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.95G/4.99G [00:46<01:11, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 1.97G/4.97G [00:46<01:12, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.96G/4.99G [00:46<01:11, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 1.98G/4.97G [00:47<01:11, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.97G/4.99G [00:47<01:10, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 1.99G/4.97G [00:47<01:11, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 1.98G/4.99G [00:47<01:10, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 2.00G/4.97G [00:47<01:10, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 1.99G/4.99G [00:47<01:10, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 2.01G/4.97G [00:47<01:11, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.02G/4.97G [00:48<01:09, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  41%|█▋  | 2.02G/4.97G [00:48<01:10, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 2.01G/4.99G [00:48<01:09, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.03G/4.97G [00:48<01:09, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▌  | 2.02G/4.99G [00:48<01:09, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.04G/4.97G [00:48<01:09, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.03G/4.99G [00:48<01:09, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.06G/4.97G [00:48<01:10, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.07G/4.97G [00:49<01:09, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.07G/4.97G [00:49<01:09, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.06G/4.99G [00:49<01:08, 42.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.08G/4.97G [00:49<01:08, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.07G/4.99G [00:49<01:08, 42.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.09G/4.97G [00:49<01:08, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.08G/4.99G [00:49<01:08, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.10G/4.97G [00:49<01:09, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.09G/4.99G [00:49<01:07, 42.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.11G/4.97G [00:50<01:08, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.10G/4.99G [00:50<01:07, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.12G/4.97G [00:50<01:08, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.11G/4.99G [00:50<01:07, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.13G/4.97G [00:50<01:07, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.12G/4.99G [00:50<01:07, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.14G/4.97G [00:50<01:08, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.13G/4.99G [00:50<01:07, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.15G/4.97G [00:51<01:07, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.14G/4.99G [00:51<01:06, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.16G/4.97G [00:51<01:06, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▋  | 2.17G/4.97G [00:51<01:06, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  44%|█▋  | 2.17G/4.97G [00:51<01:06, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.16G/4.99G [00:51<01:06, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.18G/4.97G [00:51<01:07, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.19G/4.97G [00:52<01:06, 41.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  44%|█▊  | 2.19G/4.97G [00:52<01:06, 41.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.20G/4.97G [00:52<01:05, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  44%|█▊  | 2.20G/4.97G [00:52<01:06, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.21G/4.97G [00:52<01:04, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.21G/4.97G [00:52<01:06, 41.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.22G/4.97G [00:52<01:06, 41.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.22G/4.97G [00:52<01:07, 40.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.23G/4.97G [00:53<01:05, 41.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.23G/4.97G [00:53<01:06, 41.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.24G/4.97G [00:53<01:04, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.24G/4.97G [00:53<01:05, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.25G/4.97G [00:53<01:04, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.25G/4.97G [00:53<01:05, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.26G/4.97G [00:53<01:05, 41.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.26G/4.97G [00:53<01:04, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.28G/4.97G [00:54<01:04, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.28G/4.97G [00:54<01:04, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.29G/4.97G [00:54<01:03, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.29G/4.97G [00:54<01:03, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.30G/4.97G [00:54<01:03, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.30G/4.97G [00:54<01:03, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.31G/4.97G [00:54<01:04, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.31G/4.97G [00:54<01:02, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▊  | 2.32G/4.97G [00:55<01:03, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  47%|█▊  | 2.32G/4.97G [00:55<01:02, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▊  | 2.33G/4.97G [00:55<01:02, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  47%|█▊  | 2.33G/4.97G [00:55<01:02, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▉  | 2.34G/4.97G [00:55<01:02, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  47%|█▉  | 2.34G/4.97G [00:55<01:02, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▉  | 2.35G/4.97G [00:55<01:03, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  47%|█▉  | 2.35G/4.97G [00:55<01:02, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▉  | 2.36G/4.97G [00:56<01:02, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  47%|█▉  | 2.36G/4.97G [00:56<01:01, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▉  | 2.35G/4.99G [00:56<01:02, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.37G/4.97G [00:56<01:02, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▉  | 2.36G/4.99G [00:56<01:02, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.38G/4.97G [00:56<01:01, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.39G/4.97G [00:56<01:02, 41.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.39G/4.97G [00:56<01:02, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.38G/4.99G [00:56<01:01, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.40G/4.97G [00:57<01:01, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.39G/4.99G [00:57<01:01, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.41G/4.97G [00:57<01:01, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.40G/4.99G [00:57<01:00, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.42G/4.97G [00:57<01:00, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.43G/4.97G [00:57<01:00, 41.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.43G/4.97G [00:57<01:01, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.44G/4.97G [00:58<01:00, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.44G/4.97G [00:58<01:00, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.45G/4.97G [00:58<00:59, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.45G/4.97G [00:58<00:59, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.46G/4.97G [00:58<00:59, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  50%|█▉  | 2.46G/4.97G [00:58<00:59, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.45G/4.99G [00:58<00:59, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.47G/4.97G [00:58<01:00, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.46G/4.99G [00:58<00:58, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.49G/4.97G [00:59<00:59, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|█▉  | 2.47G/4.99G [00:59<00:59, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  50%|██  | 2.50G/4.97G [00:59<00:58, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|██  | 2.51G/4.97G [00:59<00:58, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  50%|██  | 2.51G/4.97G [00:59<00:58, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|█▉  | 2.50G/4.99G [00:59<00:58, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.52G/4.97G [00:59<00:59, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.53G/4.97G [01:00<00:58, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.53G/4.97G [01:00<00:58, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.54G/4.97G [01:00<00:57, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.54G/4.97G [01:00<00:57, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.55G/4.97G [01:00<00:57, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.55G/4.97G [01:00<00:57, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.54G/4.99G [01:00<00:57, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.56G/4.97G [01:00<00:58, 41.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.55G/4.99G [01:00<00:57, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.57G/4.97G [01:01<00:57, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.56G/4.99G [01:01<00:56, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.58G/4.97G [01:01<00:57, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.57G/4.99G [01:01<00:56, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.59G/4.97G [01:01<00:56, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.58G/4.99G [01:01<00:56, 42.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.59G/4.99G [01:01<00:56, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.61G/4.97G [01:02<00:56, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.60G/4.99G [01:02<00:56, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  53%|██  | 2.61G/4.97G [01:02<00:56, 41.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  53%|██  | 2.62G/4.97G [01:02<00:55, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.61G/4.99G [01:02<00:56, 42.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██  | 2.63G/4.97G [01:02<00:55, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  53%|██  | 2.63G/4.97G [01:02<01:06, 35.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██  | 2.64G/4.97G [01:02<00:56, 41.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  53%|██▏ | 2.64G/4.97G [01:02<00:58, 39.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██▏ | 2.65G/4.97G [01:03<00:55, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  53%|██▏ | 2.65G/4.97G [01:03<00:57, 40.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.66G/4.97G [01:03<00:54, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.66G/4.97G [01:03<00:56, 40.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.67G/4.97G [01:03<00:54, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.67G/4.97G [01:03<00:56, 40.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.68G/4.97G [01:03<00:55, 41.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.68G/4.97G [01:03<00:55, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.69G/4.97G [01:04<00:54, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.69G/4.97G [01:04<00:54, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.71G/4.97G [01:04<00:53, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.71G/4.97G [01:04<00:53, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.72G/4.97G [01:04<00:53, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.72G/4.97G [01:04<00:55, 40.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.73G/4.97G [01:04<00:54, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.74G/4.97G [01:05<00:53, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.73G/4.99G [01:05<00:56, 40.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.75G/4.97G [01:05<00:52, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.74G/4.99G [01:05<00:55, 40.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.76G/4.97G [01:05<00:52, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.75G/4.99G [01:05<00:54, 41.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.76G/4.97G [01:05<00:52, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▏ | 2.77G/4.97G [01:05<00:53, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  56%|██▏ | 2.77G/4.97G [01:05<00:52, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▏ | 2.78G/4.97G [01:06<00:52, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  56%|██▏ | 2.78G/4.97G [01:06<00:51, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▏ | 2.79G/4.97G [01:06<00:51, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  56%|██▏ | 2.79G/4.97G [01:06<00:52, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▎ | 2.80G/4.97G [01:06<00:51, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  56%|██▎ | 2.80G/4.97G [01:06<00:51, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▎ | 2.81G/4.97G [01:06<00:52, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.81G/4.97G [01:06<00:50, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.82G/4.97G [01:07<00:51, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.82G/4.97G [01:07<00:50, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.83G/4.97G [01:07<00:51, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.83G/4.97G [01:07<00:50, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.84G/4.97G [01:07<00:50, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.84G/4.97G [01:07<00:49, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.85G/4.97G [01:07<00:51, 41.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.85G/4.97G [01:07<00:49, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.86G/4.97G [01:08<00:50, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.86G/4.97G [01:08<00:49, 42.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.87G/4.97G [01:08<00:49, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.87G/4.97G [01:08<00:49, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.88G/4.97G [01:08<00:49, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.88G/4.97G [01:08<00:49, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.89G/4.97G [01:08<00:50, 41.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.89G/4.97G [01:08<00:48, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.90G/4.97G [01:09<00:49, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.90G/4.97G [01:09<00:48, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.92G/4.97G [01:09<00:48, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.92G/4.97G [01:09<00:48, 42.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.93G/4.97G [01:09<00:48, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.93G/4.97G [01:09<00:47, 42.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.94G/4.97G [01:09<00:49, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.94G/4.97G [01:09<00:47, 42.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.95G/4.97G [01:10<00:48, 41.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.95G/4.97G [01:10<00:47, 42.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▍ | 2.96G/4.97G [01:10<00:48, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  59%|██▍ | 2.96G/4.97G [01:10<00:47, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 2.97G/4.97G [01:10<00:47, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.97G/4.97G [01:10<00:47, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 2.98G/4.97G [01:10<00:48, 41.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.98G/4.97G [01:10<00:47, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 2.99G/4.97G [01:11<00:47, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.99G/4.97G [01:11<00:46, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 3.00G/4.97G [01:11<00:46, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 3.00G/4.97G [01:11<00:46, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 3.01G/4.97G [01:11<00:46, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.01G/4.97G [01:11<00:46, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.02G/4.97G [01:11<00:47, 41.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.02G/4.97G [01:11<00:45, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.03G/4.97G [01:12<00:46, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.03G/4.97G [01:12<00:45, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.04G/4.97G [01:12<00:45, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.04G/4.97G [01:12<00:45, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.05G/4.97G [01:12<00:45, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.05G/4.97G [01:12<00:44, 42.7MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.05G/4.99G [01:12<00:45, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.06G/4.97G [01:12<00:46, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.06G/4.99G [01:12<00:45, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.07G/4.97G [01:13<00:45, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.07G/4.99G [01:13<00:45, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.08G/4.97G [01:13<00:45, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.08G/4.99G [01:13<00:44, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.09G/4.97G [01:13<00:44, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.09G/4.99G [01:13<00:44, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.10G/4.97G [01:13<00:45, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.10G/4.99G [01:13<00:44, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.11G/4.97G [01:14<00:44, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.11G/4.99G [01:14<00:44, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.12G/4.97G [01:14<00:43, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|██▌ | 3.12G/4.99G [01:14<00:44, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.14G/4.97G [01:14<00:43, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|██▌ | 3.14G/4.99G [01:14<00:43, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.15G/4.97G [01:14<00:44, 41.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.16G/4.97G [01:15<00:43, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  63%|██▌ | 3.16G/4.97G [01:15<00:43, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.17G/4.97G [01:15<00:42, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.17G/4.97G [01:15<00:43, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.18G/4.97G [01:15<00:42, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.18G/4.97G [01:15<00:42, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.18G/4.99G [01:15<00:45, 39.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.19G/4.97G [01:15<00:43, 41.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.20G/4.97G [01:16<00:42, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.20G/4.97G [01:16<00:42, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.21G/4.97G [01:16<00:41, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.21G/4.97G [01:16<00:41, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.21G/4.99G [01:16<00:43, 41.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.22G/4.97G [01:16<00:41, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.22G/4.99G [01:16<00:42, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.23G/4.97G [01:16<00:44, 39.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.23G/4.99G [01:16<00:41, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.24G/4.97G [01:17<00:43, 40.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.24G/4.99G [01:17<00:41, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.25G/4.97G [01:17<00:42, 40.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.25G/4.99G [01:17<00:40, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  66%|██▌ | 3.26G/4.97G [01:17<00:41, 41.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.26G/4.99G [01:17<00:40, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.27G/4.97G [01:17<00:40, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▌ | 3.27G/4.99G [01:17<00:40, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.28G/4.97G [01:18<00:40, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.28G/4.99G [01:18<00:40, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.29G/4.97G [01:18<00:39, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.29G/4.99G [01:18<00:39, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.30G/4.97G [01:18<00:39, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.30G/4.99G [01:18<00:39, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.31G/4.97G [01:18<00:38, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.31G/4.99G [01:18<00:39, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.32G/4.97G [01:19<00:38, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|██▋ | 3.32G/4.99G [01:19<00:39, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.33G/4.97G [01:19<00:38, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|██▋ | 3.33G/4.99G [01:19<00:39, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.34G/4.97G [01:19<00:38, 42.7MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|██▋ | 3.34G/4.99G [01:19<00:38, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.36G/4.97G [01:19<00:38, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.36G/4.97G [01:19<00:42, 37.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.37G/4.97G [01:20<00:38, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.37G/4.97G [01:20<00:41, 39.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.38G/4.97G [01:20<00:39, 40.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.38G/4.99G [01:20<00:43, 37.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.39G/4.97G [01:20<00:38, 40.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.39G/4.99G [01:20<00:41, 38.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.40G/4.97G [01:20<00:38, 41.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.40G/4.99G [01:20<00:40, 39.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  69%|██▋ | 3.41G/4.97G [01:21<00:37, 41.7MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.41G/4.99G [01:21<00:38, 40.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  69%|██▋ | 3.42G/4.97G [01:21<00:37, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.42G/4.99G [01:21<00:38, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  69%|██▊ | 3.43G/4.97G [01:21<00:36, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▋ | 3.43G/4.99G [01:21<00:37, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  69%|██▊ | 3.44G/4.97G [01:21<00:36, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.44G/4.99G [01:21<00:36, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  69%|██▊ | 3.45G/4.97G [01:22<00:35, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.45G/4.99G [01:22<00:36, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.46G/4.97G [01:22<00:35, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.46G/4.99G [01:22<00:36, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.47G/4.97G [01:22<00:35, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.47G/4.99G [01:22<00:35, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.48G/4.97G [01:22<00:35, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.48G/4.99G [01:22<00:35, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.49G/4.97G [01:23<00:35, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.49G/4.99G [01:23<00:35, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.50G/4.97G [01:23<00:34, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.50G/4.99G [01:23<00:34, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.51G/4.97G [01:23<00:34, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.51G/4.99G [01:23<00:34, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.52G/4.97G [01:23<00:34, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.52G/4.99G [01:23<00:34, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.53G/4.97G [01:24<00:33, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.53G/4.99G [01:24<00:34, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.54G/4.97G [01:24<00:33, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.54G/4.99G [01:24<00:33, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.55G/4.97G [01:24<00:33, 42.7MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.55G/4.99G [01:24<00:33, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  72%|██▊ | 3.57G/4.97G [01:24<00:32, 42.7MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.57G/4.99G [01:24<00:33, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  72%|██▉ | 3.58G/4.97G [01:25<00:32, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▊ | 3.58G/4.99G [01:25<00:33, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  72%|██▉ | 3.59G/4.97G [01:25<00:32, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▊ | 3.59G/4.99G [01:25<00:32, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  72%|██▉ | 3.60G/4.97G [01:25<00:32, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▉ | 3.60G/4.99G [01:25<00:32, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.61G/4.97G [01:25<00:32, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▉ | 3.61G/4.99G [01:25<00:32, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.62G/4.97G [01:26<00:31, 42.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▉ | 3.62G/4.99G [01:26<00:33, 40.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.63G/4.97G [01:26<00:31, 42.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.63G/4.99G [01:26<00:33, 41.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.64G/4.97G [01:26<00:31, 42.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.65G/4.97G [01:26<00:31, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.65G/4.97G [01:26<00:32, 41.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.65G/4.99G [01:26<00:31, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.66G/4.97G [01:27<00:30, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.66G/4.99G [01:27<00:31, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.67G/4.97G [01:27<00:30, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.67G/4.99G [01:27<00:31, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.68G/4.97G [01:27<00:30, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.68G/4.99G [01:27<00:30, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.69G/4.97G [01:27<00:30, 41.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.70G/4.97G [01:28<00:30, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.70G/4.97G [01:28<00:30, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|██▉ | 3.71G/4.97G [01:28<00:29, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  75%|██▉ | 3.71G/4.97G [01:28<00:30, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.71G/4.99G [01:28<00:29, 43.0MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  75%|██▉ | 3.72G/4.97G [01:28<00:29, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|██▉ | 3.72G/4.99G [01:28<00:29, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  75%|███ | 3.73G/4.97G [01:28<00:29, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|██▉ | 3.73G/4.99G [01:28<00:29, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  75%|███ | 3.74G/4.97G [01:29<00:29, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███ | 3.75G/4.97G [01:29<00:28, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.75G/4.97G [01:29<00:28, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.76G/4.97G [01:29<00:28, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.76G/4.97G [01:29<00:28, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|███ | 3.76G/4.99G [01:29<00:28, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.77G/4.97G [01:29<00:29, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.79G/4.97G [01:30<00:28, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.79G/4.97G [01:30<00:28, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.80G/4.97G [01:30<00:27, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.80G/4.97G [01:30<00:27, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.81G/4.97G [01:30<00:27, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.81G/4.97G [01:30<00:27, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.81G/4.99G [01:30<00:27, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.82G/4.97G [01:30<00:28, 41.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.82G/4.99G [01:30<00:27, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.83G/4.97G [01:31<00:27, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.83G/4.99G [01:31<00:27, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.84G/4.97G [01:31<00:26, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.84G/4.99G [01:31<00:27, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.85G/4.97G [01:31<00:26, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.85G/4.99G [01:31<00:26, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  78%|███ | 3.86G/4.97G [01:31<00:26, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███ | 3.87G/4.97G [01:32<00:26, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  78%|███ | 3.87G/4.97G [01:32<00:26, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███ | 3.88G/4.97G [01:32<00:25, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  78%|███ | 3.88G/4.97G [01:32<00:25, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███▏| 3.89G/4.97G [01:32<00:25, 42.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███ | 3.89G/4.99G [01:32<00:25, 43.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  78%|███▏| 3.89G/4.97G [01:32<00:25, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  78%|███▏| 3.90G/4.97G [01:32<00:25, 41.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███▏| 3.90G/4.99G [01:32<00:26, 41.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███▏| 3.91G/4.99G [01:33<00:25, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.91G/4.97G [01:33<00:25, 41.7MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.92G/4.99G [01:33<00:25, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.92G/4.97G [01:33<00:25, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.93G/4.99G [01:33<00:24, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.93G/4.97G [01:33<00:24, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  79%|███▏| 3.94G/4.97G [01:33<00:24, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.94G/4.97G [01:33<00:24, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.95G/4.97G [01:34<00:24, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 3.96G/4.97G [01:34<00:23, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.96G/4.97G [01:34<00:23, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.96G/4.99G [01:34<00:24, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.97G/4.97G [01:34<00:23, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 3.97G/4.97G [01:34<00:23, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.98G/4.97G [01:34<00:23, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 4.00G/4.97G [01:35<00:23, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 4.00G/4.97G [01:35<00:23, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▏| 4.01G/4.97G [01:35<00:22, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  81%|███▏| 4.01G/4.97G [01:35<00:22, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 4.01G/4.99G [01:35<00:23, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  81%|███▏| 4.02G/4.97G [01:35<00:22, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 4.02G/4.99G [01:35<00:23, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  81%|███▏| 4.03G/4.97G [01:35<00:22, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▏| 4.03G/4.97G [01:35<00:22, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▏| 4.04G/4.97G [01:36<00:22, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  81%|███▎| 4.05G/4.97G [01:36<00:22, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▏| 4.05G/4.99G [01:36<00:22, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.06G/4.97G [01:36<00:21, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▎| 4.06G/4.99G [01:36<00:22, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.07G/4.97G [01:36<00:21, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.07G/4.99G [01:36<00:22, 41.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.08G/4.97G [01:37<00:21, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.08G/4.97G [01:37<00:21, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.09G/4.97G [01:37<00:20, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.09G/4.97G [01:37<00:20, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.10G/4.97G [01:37<00:20, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.10G/4.97G [01:37<00:20, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.11G/4.97G [01:37<00:20, 41.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.11G/4.97G [01:37<00:20, 41.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.12G/4.97G [01:38<00:20, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.12G/4.97G [01:38<00:20, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.13G/4.97G [01:38<00:20, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.13G/4.97G [01:38<00:19, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.13G/4.99G [01:38<00:20, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.14G/4.97G [01:38<00:19, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.14G/4.99G [01:38<00:19, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.15G/4.97G [01:38<00:19, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.16G/4.97G [01:39<00:19, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  84%|███▎| 4.16G/4.97G [01:39<00:19, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.17G/4.97G [01:39<00:18, 42.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.17G/4.99G [01:39<00:19, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.18G/4.97G [01:39<00:18, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.18G/4.99G [01:39<00:18, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  84%|███▎| 4.18G/4.97G [01:39<00:18, 42.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.19G/4.97G [01:39<00:18, 41.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.19G/4.99G [01:39<00:19, 41.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.20G/4.99G [01:40<00:18, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.20G/4.97G [01:40<00:18, 41.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.22G/4.97G [01:40<00:18, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.23G/4.97G [01:40<00:17, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.23G/4.99G [01:40<00:18, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.23G/4.97G [01:40<00:17, 41.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.24G/4.97G [01:40<00:17, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.24G/4.99G [01:40<00:18, 41.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.25G/4.97G [01:41<00:17, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.25G/4.99G [01:41<00:17, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.26G/4.97G [01:41<00:16, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.26G/4.97G [01:41<00:17, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.27G/4.97G [01:41<00:16, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.27G/4.97G [01:41<00:16, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.28G/4.97G [01:41<00:16, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.28G/4.99G [01:41<00:17, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.29G/4.99G [01:42<00:16, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.29G/4.97G [01:42<00:16, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.30G/4.99G [01:42<00:16, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.30G/4.97G [01:42<00:16, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.31G/4.97G [01:42<00:15, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.31G/4.97G [01:42<00:15, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.32G/4.97G [01:42<00:15, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.32G/4.97G [01:42<00:15, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.33G/4.99G [01:43<00:15, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.34G/4.97G [01:43<00:14, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.34G/4.99G [01:43<00:15, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.35G/4.97G [01:43<00:14, 42.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.35G/4.97G [01:43<00:14, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.35G/4.99G [01:43<00:15, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.36G/4.97G [01:43<00:14, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.36G/4.99G [01:43<00:15, 41.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.37G/4.97G [01:44<00:14, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.38G/4.97G [01:44<00:14, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.38G/4.99G [01:44<00:14, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.38G/4.97G [01:44<00:14, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.39G/4.97G [01:44<00:13, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.39G/4.97G [01:44<00:13, 42.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.40G/4.97G [01:44<00:13, 41.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.41G/4.97G [01:45<00:13, 42.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.41G/4.99G [01:45<00:13, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.42G/4.97G [01:45<00:13, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.42G/4.99G [01:45<00:13, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.42G/4.97G [01:45<00:12, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.44G/4.97G [01:45<00:12, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.44G/4.97G [01:45<00:12, 42.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.45G/4.97G [01:45<00:12, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.45G/4.99G [01:45<00:13, 41.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.46G/4.97G [01:46<00:12, 41.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.46G/4.97G [01:46<00:12, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.47G/4.99G [01:46<00:12, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.48G/4.97G [01:46<00:11, 42.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.48G/4.97G [01:46<00:11, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.48G/4.99G [01:46<00:12, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.49G/4.97G [01:46<00:11, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.50G/4.97G [01:47<00:11, 42.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.50G/4.99G [01:47<00:11, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.51G/4.97G [01:47<00:11, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.51G/4.99G [01:47<00:11, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.52G/4.97G [01:47<00:10, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▌| 4.52G/4.99G [01:47<00:11, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.52G/4.97G [01:47<00:10, 42.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.53G/4.97G [01:47<00:10, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.54G/4.97G [01:48<00:10, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.54G/4.97G [01:48<00:10, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.55G/4.97G [01:48<00:10, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.55G/4.97G [01:48<00:09, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.56G/4.97G [01:48<00:09, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.56G/4.97G [01:48<00:09, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.56G/4.99G [01:48<00:11, 38.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.57G/4.97G [01:48<00:09, 41.2MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.58G/4.97G [01:49<00:09, 41.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.58G/4.97G [01:49<00:09, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.58G/4.99G [01:49<00:10, 40.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.59G/4.97G [01:49<00:09, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.60G/4.97G [01:49<00:08, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.60G/4.97G [01:49<00:08, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.60G/4.99G [01:49<00:09, 41.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.61G/4.97G [01:49<00:09, 38.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.61G/4.99G [01:49<00:09, 41.9MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.62G/4.97G [01:50<00:08, 40.1MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.62G/4.99G [01:50<00:08, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.63G/4.97G [01:50<00:08, 40.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.63G/4.99G [01:50<00:08, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.65G/4.97G [01:50<00:07, 41.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.65G/4.99G [01:50<00:08, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  94%|███▋| 4.66G/4.97G [01:50<00:07, 41.7MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.66G/4.99G [01:50<00:07, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.67G/4.97G [01:51<00:07, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.67G/4.99G [01:51<00:07, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.68G/4.97G [01:51<00:07, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▋| 4.68G/4.99G [01:51<00:07, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.69G/4.97G [01:51<00:06, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▊| 4.69G/4.99G [01:51<00:07, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.70G/4.97G [01:51<00:06, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▊| 4.70G/4.99G [01:51<00:06, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.71G/4.97G [01:52<00:06, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▊| 4.71G/4.99G [01:52<00:06, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.72G/4.97G [01:52<00:06, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.72G/4.99G [01:52<00:06, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.73G/4.97G [01:52<00:05, 42.6MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.73G/4.99G [01:52<00:06, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.74G/4.97G [01:52<00:05, 42.7MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.75G/4.97G [01:53<00:05, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.75G/4.97G [01:53<00:05, 41.6MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.76G/4.97G [01:53<00:05, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.76G/4.97G [01:53<00:05, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.76G/4.99G [01:53<00:05, 43.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.77G/4.97G [01:53<00:04, 42.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.77G/4.99G [01:53<00:05, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.78G/4.97G [01:53<00:04, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.79G/4.97G [01:54<00:04, 42.3MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.79G/4.97G [01:54<00:04, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▊| 4.80G/4.97G [01:54<00:04, 42.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  97%|███▊| 4.80G/4.97G [01:54<00:03, 42.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▊| 4.81G/4.97G [01:54<00:03, 42.6MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  97%|███▊| 4.81G/4.97G [01:54<00:03, 42.2MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.81G/4.99G [01:54<00:04, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.82G/4.97G [01:54<00:03, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▊| 4.82G/4.99G [01:54<00:03, 42.7MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.83G/4.97G [01:55<00:03, 41.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▊| 4.83G/4.99G [01:55<00:03, 42.8MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.84G/4.97G [01:55<00:03, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.85G/4.97G [01:55<00:02, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▉| 4.85G/4.99G [01:55<00:03, 43.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  98%|███▉| 4.87G/4.97G [01:55<00:02, 38.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.87G/4.97G [01:55<00:02, 41.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.88G/4.99G [01:56<00:02, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.88G/4.97G [01:56<00:02, 42.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.89G/4.97G [01:56<00:02, 42.3MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.89G/4.99G [01:56<00:02, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.90G/4.97G [01:56<00:01, 42.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.91G/4.97G [01:56<00:01, 41.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.91G/4.99G [01:56<00:02, 41.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.91G/4.97G [01:56<00:01, 40.3MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.92G/4.97G [01:57<00:01, 41.7MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.92G/4.97G [01:57<00:01, 41.1MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.93G/4.97G [01:57<00:01, 42.0MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.93G/4.97G [01:57<00:01, 41.5MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.94G/4.97G [01:57<00:00, 42.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.94G/4.97G [01:57<00:00, 41.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors: 100%|███▉| 4.95G/4.97G [01:57<00:00, 41.4MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.95G/4.97G [01:57<00:00, 41.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.96G/4.99G [01:58<00:00, 42.1MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00004.safetensors: 100%|███▉| 4.96G/4.97G [01:58<00:00, 41.8MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|███▉| 4.97G/4.99G [01:58<00:00, 42.4MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors: 100%|████| 4.97G/4.97G [01:58<00:00, 42.0MB/s]\u001b[A\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|████| 4.97G/4.97G [01:58<00:00, 42.0MB/s]\n",
      "\n",
      "model-00002-of-00004.safetensors: 100%|███▉| 4.98G/4.99G [01:58<00:00, 40.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|████| 4.99G/4.99G [01:58<00:00, 42.0MB/s]\u001b[A\n",
      "model.safetensors.index.json: 100%|█████████| 29.8k/29.8k [00:00<00:00, 169MB/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.41it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.15it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.99it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.67it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.62it/s]\n",
      "\n",
      "INFO 01-16 12:26:19 model_runner.py:1099] Loading model weights took 15.2208 GB\n",
      "INFO 01-16 12:26:21 worker.py:241] Memory profiling takes 1.53 seconds\n",
      "INFO 01-16 12:26:21 worker.py:241] the current vLLM instance can use total_gpu_memory (21.95GiB) x gpu_memory_utilization (0.90) = 19.76GiB\n",
      "INFO 01-16 12:26:21 worker.py:241] model weights take 15.22GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 0.48GiB; the rest of the memory reserved for KV Cache is 3.99GiB.\n",
      "INFO 01-16 12:26:21 gpu_executor.py:76] # GPU blocks: 1635, # CPU blocks: 1638\n",
      "INFO 01-16 12:26:21 gpu_executor.py:80] Maximum concurrency for 4096 tokens per request: 6.39x\n",
      "INFO 01-16 12:26:23 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:17<00:00,  1.96it/s]\n",
      "INFO 01-16 12:26:41 model_runner.py:1535] Graph capturing finished in 18 secs, took 0.27 GiB\n",
      "INFO 01-16 12:26:41 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 21.50 seconds\n",
      "INFO 01-16 12:26:41 api_server.py:640] Using supplied chat template:\n",
      "INFO 01-16 12:26:41 api_server.py:640] None\n",
      "INFO 01-16 12:26:41 launcher.py:19] Available routes are:\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /health, Methods: GET\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /version, Methods: GET\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /pooling, Methods: POST\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /score, Methods: POST\n",
      "INFO 01-16 12:26:41 launcher.py:27] Route: /v1/score, Methods: POST\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m946\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     ::1:46142 - \"\u001b[1mPOST //v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     ::1:57400 - \"\u001b[1mPOST //v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "INFO 01-16 12:50:41 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "INFO 01-16 12:50:41 logger.py:37] Received request chatcmpl-87b8a014db7b4792a713ac1bbcf09aad: prompt: \"<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. \\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \\nPlease ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect. \\nIf you don't know the answer to a question, please don't share false information.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>\\nHuman: Describe what is the S&P500 in 100 words or less.\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:40442 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 12:50:41 engine.py:267] Added request chatcmpl-87b8a014db7b4792a713ac1bbcf09aad.\n",
      "INFO 01-16 12:50:42 metrics.py:467] Avg prompt throughput: 18.2 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 12:50:46 logger.py:37] Received request chatcmpl-5f3e85cc9ba74fcb8bb97fc38562fd63: prompt: \"<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. \\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \\nPlease ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect. \\nIf you don't know the answer to a question, please don't share false information.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>\\nHuman: How many companies are included?\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:40442 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 12:50:47 engine.py:267] Added request chatcmpl-5f3e85cc9ba74fcb8bb97fc38562fd63.\n",
      "INFO 01-16 12:50:47 metrics.py:467] Avg prompt throughput: 30.7 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 12:51:00 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 12:51:10 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:02:54 logger.py:37] Received request chatcmpl-55e193de533f4cd1ad5bf7e8091e923a: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe.\\nYou will be asked a question, to which you must give an answer.\\nYour answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\\nPlease ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, answer \"I don\\'t know\".\\n<</SYS>>\\n\\n### QUESTION:\\nDescribe what is the S&P500 in 100 words or less.\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:47368 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:02:54 engine.py:267] Added request chatcmpl-55e193de533f4cd1ad5bf7e8091e923a.\n",
      "INFO 01-16 13:02:55 metrics.py:467] Avg prompt throughput: 38.6 tokens/s, Avg generation throughput: 3.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:03:00 logger.py:37] Received request chatcmpl-1b193b185890482e805e34a80a26b2b5: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe.\\nYou will be asked a question, to which you must give an answer.\\nYour answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\\nPlease ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\\nIf you don\\'t know the answer to a question, answer \"I don\\'t know\".\\n<</SYS>>\\n\\n### QUESTION:\\nHow many companies are included?\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:47368 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:03:00 engine.py:267] Added request chatcmpl-1b193b185890482e805e34a80a26b2b5.\n",
      "INFO 01-16 13:03:01 metrics.py:467] Avg prompt throughput: 35.7 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:03:12 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:03:22 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:03:49 logger.py:37] Received request chatcmpl-461a928b329047f3b98807142aa10bbf: prompt: \"<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect. If you don't know the answer to a question, please don't share false information.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>\\nHuman: Describe what is the S&P500 in 100 words or less.\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:50974 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:03:49 engine.py:267] Added request chatcmpl-461a928b329047f3b98807142aa10bbf.\n",
      "INFO 01-16 13:03:50 metrics.py:467] Avg prompt throughput: 22.2 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:03:55 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:03:55 logger.py:37] Received request chatcmpl-9b776c5790de4c5e9ff990c637ce4baf: prompt: '<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect. If you don\\'t know the answer to a question, please don\\'t share false information.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>Human: Describe what is the S&P500 in 100 words or less.\\nAI: The S&P 500 is a stock market index that tracks the performance of 500 large companies listed on stock exchanges in the United States. It\\'s widely used as a benchmark for the U.S. stock market and is often referred to as the \"S&P\" or \"the Index\". The index is market-capitalization weighted, meaning that larger companies have a greater influence on the index\\'s movement.\\nHuman: How many companies are included?\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:50974 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:03:56 engine.py:267] Added request chatcmpl-9b776c5790de4c5e9ff990c637ce4baf.\n",
      "INFO 01-16 13:03:57 logger.py:37] Received request chatcmpl-f3dc3bbf58ea45e59e65014bb4f448a0: prompt: \"<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect. If you don't know the answer to a question, please don't share false information.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>\\nHuman: Describe what is the S&P500 results of the last 5 starting with the 2024\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:50974 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:03:57 engine.py:267] Added request chatcmpl-f3dc3bbf58ea45e59e65014bb4f448a0.\n",
      "INFO 01-16 13:04:00 metrics.py:467] Avg prompt throughput: 84.5 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:04:04 logger.py:37] Received request chatcmpl-7ead48f6ec2b432eb7c2a83a28453e17: prompt: \"<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect. If you don't know the answer to a question, please don't share false information.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>Human: Describe what is the S&P500 results of the last 5 starting with the 2024\\nAI: I'm sorry for the confusion, but as of now, we are in the year 2023. Therefore, I cannot provide the results of the S&P500 for the years 2024 and beyond, as these results do not exist yet. The S&P500 is a stock market index that tracks the performance of 500 large companies listed on stock exchanges in the United States. Its historical data is available up to the present day.\\nHuman: Describe what is the S&P500 results of the last 5 starting with the 2024\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:50974 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:04:04 engine.py:267] Added request chatcmpl-7ead48f6ec2b432eb7c2a83a28453e17.\n",
      "INFO 01-16 13:04:05 metrics.py:467] Avg prompt throughput: 58.7 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:04:19 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:04:29 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:04:54 logger.py:37] Received request chatcmpl-bba4205a65c7431d8f061607a00d391f: prompt: \"<|start_of_role|>user<|end_of_role|><s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nCurrent conversation:\\n\\nHuman: Describe what is the S&P500 in 100 words or less.\\nAI:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:32780 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:04:54 engine.py:267] Added request chatcmpl-bba4205a65c7431d8f061607a00d391f.\n",
      "INFO 01-16 13:04:55 metrics.py:467] Avg prompt throughput: 34.4 tokens/s, Avg generation throughput: 2.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:04:59 logger.py:37] Received request chatcmpl-a43633b61dd54a788bd3a4d6c6408dca: prompt: \"<|start_of_role|>user<|end_of_role|><s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nCurrent conversation:\\nHuman: Describe what is the S&P500 in 100 words or less.\\nAI: The S&P 500 is a stock market index that tracks the performance of 500 large companies listed on stock exchanges in the United States. It is one of the most widely followed equity indices and is often used as a benchmark for the overall U.S. stock market. The index is market-capitalization weighted, meaning that larger companies have a greater influence on the index's performance.\\nHuman: How many companies are included?\\nAI:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:32780 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:04:59 engine.py:267] Added request chatcmpl-a43633b61dd54a788bd3a4d6c6408dca.\n",
      "INFO 01-16 13:05:00 metrics.py:467] Avg prompt throughput: 53.4 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:05:01 logger.py:37] Received request chatcmpl-0f9d0d24a4244887860e35b794de02bf: prompt: \"<|start_of_role|>user<|end_of_role|><s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nCurrent conversation:\\n\\nHuman: Describe what is the S&P500 results of the last 5 starting with the 2024\\nAI:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:32780 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:05:01 engine.py:267] Added request chatcmpl-0f9d0d24a4244887860e35b794de02bf.\n",
      "INFO 01-16 13:05:05 metrics.py:467] Avg prompt throughput: 35.1 tokens/s, Avg generation throughput: 14.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:05:08 logger.py:37] Received request chatcmpl-d1ac9884de1e4f18994265044eaa3164: prompt: \"<|start_of_role|>user<|end_of_role|><s>[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nCurrent conversation:\\nHuman: Describe what is the S&P500 results of the last 5 starting with the 2024\\nAI: I'm sorry for the confusion, but as of now, we are in the year 2023. Therefore, I cannot provide the S&P500 results for the years 2024 and beyond, as they have not occurred yet. The S&P500 is a stock market index that tracks the performance of 500 large companies listed on stock exchanges in the United States. Its historical data is available for past years, but future results are not predictable.\\nHuman: Describe what is the S&P500 results of the last 5 starting with the 2024\\nAI:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:32780 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:05:08 engine.py:267] Added request chatcmpl-d1ac9884de1e4f18994265044eaa3164.\n",
      "INFO 01-16 13:05:10 metrics.py:467] Avg prompt throughput: 61.7 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:05:24 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:05:34 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:10:27 logger.py:37] Received request chatcmpl-d2da93da84e04642a6916f56c19bd4ae: prompt: '<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make sense or is not factually coherent, explain why instead of providing incorrect information. If you don\\'t know the answer to a question, do not share false information.\\n\\n---\\n\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary. \\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": { \\n    \"key\": \"Value inputs to the tool in valid JSON format.\"\\n  }\\n}\\n\\nYou must always ensure the inputs are correct for the tool you call. Provide all output in JSON format.\\n\\nBe sure that is in JSON format the output, nothing else. Not repeate the json again.\\n\\nThe available tools include:\\n\\nduckduckgo_search - A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args: {{\\'query\\': {{\\'description\\': \\'search query to look up\\', \\'title\\': \\'Query\\', \\'type\\': \\'string\\'}}}}\\n\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>\\nHuman: What are the last 5 year results of the S&P500?\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42480 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:10:27 engine.py:267] Added request chatcmpl-d2da93da84e04642a6916f56c19bd4ae.\n",
      "INFO 01-16 13:10:29 metrics.py:467] Avg prompt throughput: 72.2 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:10:41 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:10:51 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:16:25 logger.py:37] Received request cmpl-3e3e9ddbe0934fb5bbf87249e9ad78d8-0: prompt: '<s>[INST] <<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make sense or is not factually coherent, explain why instead of providing incorrect information. If you don\\'t know the answer to a question, do not share false information.\\n\\n---\\n\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary. \\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": { \\n    \"key\": \"Value inputs to the tool in valid JSON format.\"\\n  }\\n}\\n\\nYou must always ensure the inputs are correct for the tool you call. Provide all output in JSON format.\\n\\nBe sure that is in JSON format the output, nothing else. Not repeate the json again.\\n\\nThe available tools include:\\n\\nduckduckgo_search - A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args: {{\\'query\\': {{\\'description\\': \\'search query to look up\\', \\'title\\': \\'Query\\', \\'type\\': \\'string\\'}}}}\\n\\n<<SYS>>\\nHuman: What are the last 5 year results of the S&P500?\\nAI:\\n[/INST]\\n', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [46, 101, 8828, 6585, 79, 1264, 12511, 2005, 203, 4282, 884, 312, 17247, 30, 10157, 2790, 30, 461, 41211, 47330, 32, 25144, 526, 619, 17247, 619, 4195, 30, 2218, 3998, 10061, 32, 10604, 19969, 1395, 646, 2305, 1346, 41797, 2790, 30, 707, 9968, 1578, 30, 40605, 427, 30, 27082, 427, 30, 48509, 295, 30, 37116, 30, 556, 24220, 1795, 32, 5294, 7279, 688, 1370, 14812, 884, 309, 3098, 1247, 707, 6506, 16109, 461, 11557, 328, 24406, 32, 203, 203, 2797, 312, 7000, 1957, 646, 1930, 10568, 556, 438, 646, 8434, 3405, 2098, 42263, 30, 14189, 6916, 4174, 432, 15484, 12479, 2471, 32, 1670, 844, 2800, 1330, 3702, 322, 7592, 372, 312, 7000, 30, 745, 646, 8623, 901, 2471, 32, 203, 203, 1559, 203, 203, 4282, 1159, 2857, 372, 10204, 8684, 372, 34916, 844, 328, 9631, 299, 8253, 32, 2448, 884, 18392, 436, 35836, 1412, 372, 793, 3026, 461, 1395, 1626, 2835, 8640, 8253, 1991, 1361, 8246, 415, 7506, 32, 1659, 203, 5786, 1471, 8684, 30, 1976, 458, 2179, 44, 203, 203, 109, 334, 313, 1141, 563, 313, 31531, 322, 4559, 844, 2637, 372, 793, 4414, 334, 313, 1141, 81, 1131, 563, 301, 3701, 313, 678, 563, 313, 882, 7806, 372, 322, 4559, 328, 2080, 3398, 2179, 3107, 334, 320, 203, 111, 203, 203, 4282, 2298, 5182, 7279, 322, 7806, 884, 3809, 436, 322, 4559, 844, 1476, 32, 14112, 1169, 1688, 328, 3398, 2179, 32, 203, 203, 1467, 3654, 688, 438, 328, 3398, 2179, 322, 1688, 30, 7553, 813, 32, 3182, 29645, 332, 322, 2361, 4056, 32, 203, 203, 1318, 3304, 8684, 2305, 44, 203, 203, 32546, 32546, 1704, 81, 1903, 429, 399, 7767, 6835, 475, 7194, 47945, 6098, 7422, 32, 36235, 436, 1412, 844, 1849, 372, 7592, 10017, 2625, 1550, 5216, 32, 4237, 1395, 526, 312, 3233, 2467, 4317, 2181, 44, 3335, 25, 1217, 950, 3335, 25, 2310, 950, 330, 1903, 2467, 372, 2779, 973, 370, 330, 1311, 950, 330, 1625, 370, 330, 681, 950, 330, 800, 21028, 1540, 203, 203, 3571, 12511, 2005, 203, 21390, 44, 6180, 884, 322, 2401, 225, 39, 4947, 3276, 432, 322, 358, 24, 66, 39, 34, 34, 49, 203, 6218, 44, 203, 36265, 6585, 79, 203], lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:38782 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:16:25 engine.py:267] Added request cmpl-3e3e9ddbe0934fb5bbf87249e9ad78d8-0.\n",
      "INFO 01-16 13:16:27 metrics.py:467] Avg prompt throughput: 72.1 tokens/s, Avg generation throughput: 3.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:16:31 logger.py:37] Received request cmpl-ab72ac52340f4790a96f21880df4b68c-0: prompt: '<s>[INST] <<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make sense or is not factually coherent, explain why instead of providing incorrect information. If you don\\'t know the answer to a question, do not share false information.\\n\\n---\\n\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary. \\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": { \\n    \"key\": \"Value inputs to the tool in valid JSON format.\"\\n  }\\n}\\n\\nYou must always ensure the inputs are correct for the tool you call. Provide all output in JSON format.\\n\\nBe sure that is in JSON format the output, nothing else. Not repeate the json again.\\n\\nThe available tools include:\\n\\nduckduckgo_search - A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query., args: {{\\'query\\': {{\\'description\\': \\'search query to look up\\', \\'title\\': \\'Query\\', \\'type\\': \\'string\\'}}}}\\n\\n<<SYS>>\\nHuman: Based on the following search result, please Summarize and extract the key information of the text. Give me a sentence with that as an answer, not in json, just plain text.,What are the last 5 year results of the S&P500?:\\n\\n5 year chart of the S&P 500 stock index* The 5 year chart of S&P 500 summarizes the chages in the price well, however, we recommend to have a look at the chart(s) below, too. Similar charts of the past 10 year can be found here. You can find charts of other periods (1, 3, 6, 12 months etc.) here. 5 years return and graph of S&P 500* S&P 500 5 Year Return is at 82.05%, compared to 92.05% last month and 90.27% last year. This is higher than the long term average of 46.44%. The S&P 500 5 Year Return is the investment return received for a 5 year period, excluding dividends, when holding the S&P 500 index. Here is the summary data of average yearly returns for the S&P 500 over the last 5 years to 150 years. Years Averaged (as of end of December 2024) Stock Market Average Return per Year (Dividends Reinvested) Average Return with Dividends Reinvested & Inflation Adjusted: 150 Years: 9.381%: 6.994%: 100 Years: 10.569%: 7.405%: 50 Years: 12.393%: 8. ... This widget shows the number of times this symbol reached a new low price for specific periods, from the past 5-Days to the past 20-Years. For each period, the \"Percent From Last\" column shows you where the current price is in relationship to the Low price for that period. Directly the 5 years chart of S&P 500 index (GSPC). S&P 500 index value on 5 years graph and in other time ranges. Check it out now!>>\\nAI:\\n[/INST]\\n', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [46, 101, 8828, 6585, 79, 1264, 12511, 2005, 203, 4282, 884, 312, 17247, 30, 10157, 2790, 30, 461, 41211, 47330, 32, 25144, 526, 619, 17247, 619, 4195, 30, 2218, 3998, 10061, 32, 10604, 19969, 1395, 646, 2305, 1346, 41797, 2790, 30, 707, 9968, 1578, 30, 40605, 427, 30, 27082, 427, 30, 48509, 295, 30, 37116, 30, 556, 24220, 1795, 32, 5294, 7279, 688, 1370, 14812, 884, 309, 3098, 1247, 707, 6506, 16109, 461, 11557, 328, 24406, 32, 203, 203, 2797, 312, 7000, 1957, 646, 1930, 10568, 556, 438, 646, 8434, 3405, 2098, 42263, 30, 14189, 6916, 4174, 432, 15484, 12479, 2471, 32, 1670, 844, 2800, 1330, 3702, 322, 7592, 372, 312, 7000, 30, 745, 646, 8623, 901, 2471, 32, 203, 203, 1559, 203, 203, 4282, 1159, 2857, 372, 10204, 8684, 372, 34916, 844, 328, 9631, 299, 8253, 32, 2448, 884, 18392, 436, 35836, 1412, 372, 793, 3026, 461, 1395, 1626, 2835, 8640, 8253, 1991, 1361, 8246, 415, 7506, 32, 1659, 203, 5786, 1471, 8684, 30, 1976, 458, 2179, 44, 203, 203, 109, 334, 313, 1141, 563, 313, 31531, 322, 4559, 844, 2637, 372, 793, 4414, 334, 313, 1141, 81, 1131, 563, 301, 3701, 313, 678, 563, 313, 882, 7806, 372, 322, 4559, 328, 2080, 3398, 2179, 3107, 334, 320, 203, 111, 203, 203, 4282, 2298, 5182, 7279, 322, 7806, 884, 3809, 436, 322, 4559, 844, 1476, 32, 14112, 1169, 1688, 328, 3398, 2179, 32, 203, 203, 1467, 3654, 688, 438, 328, 3398, 2179, 322, 1688, 30, 7553, 813, 32, 3182, 29645, 332, 322, 2361, 4056, 32, 203, 203, 1318, 3304, 8684, 2305, 44, 203, 203, 32546, 32546, 1704, 81, 1903, 429, 399, 7767, 6835, 475, 7194, 47945, 6098, 7422, 32, 36235, 436, 1412, 844, 1849, 372, 7592, 10017, 2625, 1550, 5216, 32, 4237, 1395, 526, 312, 3233, 2467, 4317, 2181, 44, 3335, 25, 1217, 950, 3335, 25, 2310, 950, 330, 1903, 2467, 372, 2779, 973, 370, 330, 1311, 950, 330, 1625, 370, 330, 681, 950, 330, 800, 21028, 1540, 203, 203, 3571, 12511, 2005, 203, 21390, 44, 24226, 544, 322, 2412, 3233, 1056, 30, 4322, 10720, 3303, 518, 461, 6571, 322, 1202, 2471, 432, 322, 1344, 32, 31588, 597, 312, 15430, 623, 688, 619, 600, 7592, 30, 646, 328, 2361, 30, 2258, 14917, 1344, 4317, 8197, 884, 322, 2401, 225, 39, 4947, 3276, 432, 322, 358, 24, 66, 39, 34, 34, 2805, 203, 203, 39, 4947, 7565, 432, 322, 358, 24, 66, 225, 39, 34, 34, 12994, 1650, 28, 886, 225, 39, 4947, 7565, 432, 358, 24, 66, 225, 39, 34, 34, 23943, 5052, 322, 663, 8522, 328, 322, 7661, 4487, 30, 11510, 30, 996, 13152, 372, 1159, 312, 2779, 821, 322, 7565, 26, 101, 27, 4794, 30, 5133, 32, 26683, 29397, 432, 322, 11169, 225, 35, 34, 4947, 883, 526, 2431, 2442, 32, 2448, 883, 2290, 29397, 432, 1604, 34533, 308, 35, 30, 225, 37, 30, 225, 40, 30, 225, 35, 36, 17723, 5774, 7447, 2442, 32, 225, 39, 11274, 442, 461, 3996, 432, 358, 24, 66, 225, 39, 34, 34, 28, 358, 24, 66, 225, 39, 34, 34, 225, 39, 18526, 2744, 438, 821, 225, 42, 36, 32, 34, 39, 12556, 19112, 372, 225, 43, 36, 32, 34, 39, 23, 2401, 6811, 461, 225, 43, 34, 32, 36, 41, 23, 2401, 4947, 32, 1348, 438, 12224, 2784, 322, 1946, 5615, 10858, 432, 225, 38, 40, 32, 38, 38, 20780, 886, 358, 24, 66, 225, 39, 34, 34, 225, 39, 18526, 2744, 438, 322, 43936, 442, 6584, 436, 312, 225, 39, 4947, 7918, 30, 49074, 4451, 314, 1494, 30, 1412, 21843, 322, 358, 24, 66, 225, 39, 34, 34, 1650, 32, 9540, 438, 322, 8822, 706, 432, 10858, 4947, 631, 3217, 436, 322, 358, 24, 66, 225, 39, 34, 34, 2288, 322, 2401, 225, 39, 11274, 372, 225, 35, 39, 34, 11274, 32, 18526, 101, 399, 424, 4838, 308, 302, 432, 943, 432, 38735, 225, 36, 34, 36, 38, 27, 22454, 24899, 26891, 2744, 1292, 18526, 308, 6775, 314, 1494, 902, 5817, 5174, 27, 26891, 2744, 623, 17536, 314, 1494, 902, 5817, 5174, 549, 701, 1306, 367, 25556, 318, 44, 225, 35, 39, 34, 18526, 101, 44, 225, 43, 32, 37, 42, 35, 23, 44, 225, 40, 32, 43, 43, 38, 23, 44, 225, 35, 34, 34, 18526, 101, 44, 225, 35, 34, 32, 39, 40, 43, 23, 44, 225, 41, 32, 38, 34, 39, 23, 44, 225, 39, 34, 18526, 101, 44, 225, 35, 36, 32, 37, 43, 37, 23, 44, 225, 42, 32, 2417, 1348, 6852, 9830, 322, 1451, 432, 6496, 458, 6977, 16673, 312, 537, 7461, 7661, 436, 2818, 34533, 30, 645, 322, 11169, 225, 39, 31, 11946, 372, 322, 11169, 225, 36, 34, 31, 33221, 32, 2616, 2504, 7918, 30, 322, 313, 12240, 8649, 11313, 20, 2623, 9830, 844, 2154, 322, 1550, 7661, 438, 328, 12112, 372, 322, 20332, 7661, 436, 688, 7918, 32, 13254, 631, 322, 225, 39, 11274, 7565, 432, 358, 24, 66, 225, 39, 34, 34, 1650, 308, 57, 49084, 547, 358, 24, 66, 225, 39, 34, 34, 1650, 786, 544, 225, 39, 11274, 3996, 461, 328, 1604, 1133, 18673, 32, 3456, 561, 963, 2464, 19, 2005, 203, 6218, 44, 203, 36265, 6585, 79, 203], lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:38782 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:16:31 engine.py:267] Added request cmpl-ab72ac52340f4790a96f21880df4b68c-0.\n",
      "INFO 01-16 13:16:32 metrics.py:467] Avg prompt throughput: 171.1 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:16:45 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:16:55 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:24:45 logger.py:37] Received request chatcmpl-8e1d928a18da49b086eea078d8459d66: prompt: \"<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible, while being safe. \\nYour answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \\nPlease ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect. \\nIf you don't know the answer to a question, please don't share false information.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>\\nHuman: I want to design a finantial report around the report from the larger FSIs. What I should focus?\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>\", params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:55432 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:24:45 engine.py:267] Added request chatcmpl-8e1d928a18da49b086eea078d8459d66.\n",
      "INFO 01-16 13:24:50 metrics.py:467] Avg prompt throughput: 34.1 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:24:55 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:25:00 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:25:05 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:25:08 logger.py:37] Received request chatcmpl-04d29f4a85c54ee8bf4a74dcf390816b: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou will be asked a question, to which you must give an answer.\\nYour answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\\nPlease ensure that your responses are socially unbiased and positive in nature.\\nWhen answering, think step-by-step and explain your reasoning before providing the final answer.\\nExplain it with bullet points if it\\'s needed but using reasoning and step-by-step.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect.\\nIf you don\\'t know the answer to a question, answer \"I don\\'t know\".\\n<</SYS>>\\n\\n### QUESTION:\\nI want to design a finantial report around the report from the larger FSIs. What I should focus?\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:55432 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:25:08 engine.py:267] Added request chatcmpl-04d29f4a85c54ee8bf4a74dcf390816b.\n",
      "INFO 01-16 13:25:10 metrics.py:467] Avg prompt throughput: 47.3 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:25:15 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:25:20 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:25:25 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:25:39 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:25:49 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:35:08 logger.py:37] Received request chatcmpl-512cb7361c1d408fa23e078d358c2874: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe.\\nYou will be asked a question, to which you must give an answer.\\nYour answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\\nPlease ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct but in a simple way and very concise\\nIf you don\\'t know the answer to a question, answer \"I don\\'t know\".\\n<</SYS>>\\n\\n### QUESTION:\\nI want to design a finantial report around the report from the larger FSIs. What I should focus?\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35878 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:35:08 engine.py:267] Added request chatcmpl-512cb7361c1d408fa23e078d358c2874.\n",
      "INFO 01-16 13:35:08 metrics.py:467] Avg prompt throughput: 22.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:35:13 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:35:18 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:35:23 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:35:28 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:35:33 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:35:36 logger.py:37] Received request chatcmpl-611e1983ffae4e1ca12a5464acfd8e1d: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou will be asked a question, to which you must give an answer.\\nYour answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\\nPlease ensure that your responses are socially unbiased and positive in nature.\\nWhen answering, think step-by-step and explain your reasoning before providing the final answer.\\nExplain it with bullet points if it\\'s needed but using reasoning and step-by-step.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something incorrect.\\nIf you don\\'t know the answer to a question, answer \"I don\\'t know\".\\n<</SYS>>\\n\\n### QUESTION:\\nI want to design a finantial report around the report from the larger FSIs. What I should focus?\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.01, top_p=0.92, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:35878 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:35:36 engine.py:267] Added request chatcmpl-611e1983ffae4e1ca12a5464acfd8e1d.\n",
      "INFO 01-16 13:35:38 metrics.py:467] Avg prompt throughput: 47.2 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:35:43 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:35:48 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:35:53 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:36:06 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:36:16 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:43:34 logger.py:37] Received request chatcmpl-77ddcb808f2a41e6bf48da40bb585532: prompt: '<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe. \\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or more accurate response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 2000$ of cost per report. Obtain the revenue per year\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n### ANSWER:\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:51496 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:43:34 engine.py:267] Added request chatcmpl-77ddcb808f2a41e6bf48da40bb585532.\n",
      "INFO 01-16 13:43:34 metrics.py:467] Avg prompt throughput: 57.6 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:43:39 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:43:42 logger.py:37] Received request chatcmpl-bfdd0a4c913f4c4b9ad2ebb610e0dd18: prompt: '<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe. \\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or more accurate response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>\\n### QUESTION:\\n{\"observation\": \"daily_revenue = 3000, annual_revenue = 1095000, monthly_revenue = None\"}\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n### ANSWER:\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:51496 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:43:42 engine.py:267] Added request chatcmpl-bfdd0a4c913f4c4b9ad2ebb610e0dd18.\n",
      "INFO 01-16 13:43:44 metrics.py:467] Avg prompt throughput: 84.7 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:43:49 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:43:59 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:44:09 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:46:54 logger.py:37] Received request chatcmpl-19e8f50177d6463399952e659a072392: prompt: '<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe. \\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or more accurate response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 2000$ of cost per report. Obtain the revenue per year\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n### ANSWER:\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:59068 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:46:54 engine.py:267] Added request chatcmpl-19e8f50177d6463399952e659a072392.\n",
      "INFO 01-16 13:46:55 metrics.py:467] Avg prompt throughput: 85.3 tokens/s, Avg generation throughput: 2.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:47:00 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:47:02 logger.py:37] Received request chatcmpl-b3c96c3b23a24690a3ac607b40e60327: prompt: '<|start_of_role|>user<|end_of_role|><|start_of_role|>system<|end_of_role|>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe. \\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or more accurate response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n<|end_of_text|>\\n\\n<|start_of_role|>user<|end_of_role|>\\n### QUESTION:\\n{\"observation\": \"daily_revenue = 3000, annual_revenue = 1095000, monthly_revenue = None\"}\\n<|end_of_text|>\\n\\n<|start_of_role|>assistant<|end_of_role|>\\n### ANSWER:\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:59068 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:47:02 engine.py:267] Added request chatcmpl-b3c96c3b23a24690a3ac607b40e60327.\n",
      "INFO 01-16 13:47:05 metrics.py:467] Avg prompt throughput: 84.8 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:47:19 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 4.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:47:29 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:06 logger.py:37] Received request chatcmpl-70e04b4ccd294d2ca74cad35e0081171: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:57:06 engine.py:267] Added request chatcmpl-70e04b4ccd294d2ca74cad35e0081171.\n",
      "INFO 01-16 13:57:06 metrics.py:467] Avg prompt throughput: 68.8 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:11 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:14 logger.py:37] Received request chatcmpl-0848f734cd8346ebab40659f394cc1d5: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:57:14 engine.py:267] Added request chatcmpl-0848f734cd8346ebab40659f394cc1d5.\n",
      "INFO 01-16 13:57:16 metrics.py:467] Avg prompt throughput: 85.4 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:21 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:22 logger.py:37] Received request chatcmpl-5f3a6534878444158499e0999d57cbe9: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:57:22 engine.py:267] Added request chatcmpl-5f3a6534878444158499e0999d57cbe9.\n",
      "INFO 01-16 13:57:26 metrics.py:467] Avg prompt throughput: 85.3 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:31 logger.py:37] Received request chatcmpl-d58899b0f76a4f63a7c83e689d02fa47: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:57:31 engine.py:267] Added request chatcmpl-d58899b0f76a4f63a7c83e689d02fa47.\n",
      "INFO 01-16 13:57:31 metrics.py:467] Avg prompt throughput: 86.4 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:36 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:39 logger.py:37] Received request chatcmpl-afb18f85e1b042389ba91ef4229e61d6: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:57:39 engine.py:267] Added request chatcmpl-afb18f85e1b042389ba91ef4229e61d6.\n",
      "INFO 01-16 13:57:41 metrics.py:467] Avg prompt throughput: 86.4 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:46 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:47 logger.py:37] Received request chatcmpl-e9c9967d1bfc4b2c807605dd3080b751: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:57:47 engine.py:267] Added request chatcmpl-e9c9967d1bfc4b2c807605dd3080b751.\n",
      "INFO 01-16 13:57:51 metrics.py:467] Avg prompt throughput: 86.3 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:57:56 logger.py:37] Received request chatcmpl-23cac5e4a0874769b452fb0fe7af1fc2: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:57:56 engine.py:267] Added request chatcmpl-23cac5e4a0874769b452fb0fe7af1fc2.\n",
      "INFO 01-16 13:57:56 metrics.py:467] Avg prompt throughput: 86.2 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:01 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:04 logger.py:37] Received request chatcmpl-ed3e7812a06f4026a7a6e419c60bcf2b: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:58:04 engine.py:267] Added request chatcmpl-ed3e7812a06f4026a7a6e419c60bcf2b.\n",
      "INFO 01-16 13:58:06 metrics.py:467] Avg prompt throughput: 86.2 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:11 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:12 logger.py:37] Received request chatcmpl-2485ed791bee47f4b6e7cdb2b2195004: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:58:12 engine.py:267] Added request chatcmpl-2485ed791bee47f4b6e7cdb2b2195004.\n",
      "INFO 01-16 13:58:16 metrics.py:467] Avg prompt throughput: 86.1 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:21 logger.py:37] Received request chatcmpl-4b0d73b44679467db726dbb16fa4d7c1: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:58:21 engine.py:267] Added request chatcmpl-4b0d73b44679467db726dbb16fa4d7c1.\n",
      "INFO 01-16 13:58:21 metrics.py:467] Avg prompt throughput: 86.1 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:26 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:29 logger.py:37] Received request chatcmpl-fa17b4fe756043dbb7d47933e188834b: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:58:29 engine.py:267] Added request chatcmpl-fa17b4fe756043dbb7d47933e188834b.\n",
      "INFO 01-16 13:58:31 metrics.py:467] Avg prompt throughput: 86.1 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:36 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:37 logger.py:37] Received request chatcmpl-dc6f428eda8b43f5bced2d70e4137e5e: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:58:37 engine.py:267] Added request chatcmpl-dc6f428eda8b43f5bced2d70e4137e5e.\n",
      "INFO 01-16 13:58:41 metrics.py:467] Avg prompt throughput: 86.1 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:46 logger.py:37] Received request chatcmpl-5a796b5333a04b05a4d08b816a9b3d00: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:58:46 engine.py:267] Added request chatcmpl-5a796b5333a04b05a4d08b816a9b3d00.\n",
      "INFO 01-16 13:58:46 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:51 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:58:54 logger.py:37] Received request chatcmpl-b958a47ecbd445e6ba54d55f92d87122: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:58:54 engine.py:267] Added request chatcmpl-b958a47ecbd445e6ba54d55f92d87122.\n",
      "INFO 01-16 13:58:56 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:01 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:02 logger.py:37] Received request chatcmpl-875b87f93b6d474abb22394d45273255: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:59:02 engine.py:267] Added request chatcmpl-875b87f93b6d474abb22394d45273255.\n",
      "INFO 01-16 13:59:06 metrics.py:467] Avg prompt throughput: 86.2 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:11 logger.py:37] Received request chatcmpl-0b119a16020c49ee9c03e63b767f234d: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:59:11 engine.py:267] Added request chatcmpl-0b119a16020c49ee9c03e63b767f234d.\n",
      "INFO 01-16 13:59:11 metrics.py:467] Avg prompt throughput: 85.8 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:16 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:19 logger.py:37] Received request chatcmpl-15a1a09ea1d84408af08cf4e8ed30ce2: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:59:19 engine.py:267] Added request chatcmpl-15a1a09ea1d84408af08cf4e8ed30ce2.\n",
      "INFO 01-16 13:59:21 metrics.py:467] Avg prompt throughput: 85.8 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:27 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:28 logger.py:37] Received request chatcmpl-0a080173aa964bdb84c9a282854206f9: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:59:28 engine.py:267] Added request chatcmpl-0a080173aa964bdb84c9a282854206f9.\n",
      "INFO 01-16 13:59:32 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:36 logger.py:37] Received request chatcmpl-96e3b3fbae624cffa6e188a9e129c7d4: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:59:36 engine.py:267] Added request chatcmpl-96e3b3fbae624cffa6e188a9e129c7d4.\n",
      "INFO 01-16 13:59:37 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:42 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:44 logger.py:37] Received request chatcmpl-ac188c6fd616457a8c335e0ddda81b18: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:59:44 engine.py:267] Added request chatcmpl-ac188c6fd616457a8c335e0ddda81b18.\n",
      "INFO 01-16 13:59:47 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:52 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 13:59:53 logger.py:37] Received request chatcmpl-2f4d0b6d30844855b7db97d3778d29c1: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 13:59:53 engine.py:267] Added request chatcmpl-2f4d0b6d30844855b7db97d3778d29c1.\n",
      "INFO 01-16 13:59:57 metrics.py:467] Avg prompt throughput: 85.8 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:01 logger.py:37] Received request chatcmpl-eaf34e13237b46feaacecec521664075: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:00:01 engine.py:267] Added request chatcmpl-eaf34e13237b46feaacecec521664075.\n",
      "INFO 01-16 14:00:02 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:07 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:09 logger.py:37] Received request chatcmpl-0fbe6b8da24547ef96e8e9775754ba37: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:00:09 engine.py:267] Added request chatcmpl-0fbe6b8da24547ef96e8e9775754ba37.\n",
      "INFO 01-16 14:00:12 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:17 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:18 logger.py:37] Received request chatcmpl-abc86dba42d14428a62ddb1b16343a62: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:00:18 engine.py:267] Added request chatcmpl-abc86dba42d14428a62ddb1b16343a62.\n",
      "INFO 01-16 14:00:22 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:26 logger.py:37] Received request chatcmpl-40ba400530da4edabd8f396aea76c70f: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:00:26 engine.py:267] Added request chatcmpl-40ba400530da4edabd8f396aea76c70f.\n",
      "INFO 01-16 14:00:27 metrics.py:467] Avg prompt throughput: 85.8 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:32 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:34 logger.py:37] Received request chatcmpl-c99ec90912a24c0998cfdadd2e754b7d: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:00:35 engine.py:267] Added request chatcmpl-c99ec90912a24c0998cfdadd2e754b7d.\n",
      "INFO 01-16 14:00:37 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:42 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:43 logger.py:37] Received request chatcmpl-046a1c210a824b38889018fb99c1cc98: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:00:43 engine.py:267] Added request chatcmpl-046a1c210a824b38889018fb99c1cc98.\n",
      "INFO 01-16 14:00:47 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:51 logger.py:37] Received request chatcmpl-9959ca77472e45cea6efb8b3e1f1cec0: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:00:51 engine.py:267] Added request chatcmpl-9959ca77472e45cea6efb8b3e1f1cec0.\n",
      "INFO 01-16 14:00:52 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:00:57 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:00 logger.py:37] Received request chatcmpl-36c53c166f86462ba1e8def57acc3638: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:01:00 engine.py:267] Added request chatcmpl-36c53c166f86462ba1e8def57acc3638.\n",
      "INFO 01-16 14:01:02 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:07 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:08 logger.py:37] Received request chatcmpl-9e0f5cba88c74c649ee3ba42a49da1a1: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:01:08 engine.py:267] Added request chatcmpl-9e0f5cba88c74c649ee3ba42a49da1a1.\n",
      "INFO 01-16 14:01:12 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:16 logger.py:37] Received request chatcmpl-e585e398c66240d89dd2e26c29ef846d: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:01:16 engine.py:267] Added request chatcmpl-e585e398c66240d89dd2e26c29ef846d.\n",
      "INFO 01-16 14:01:17 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:22 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:25 logger.py:37] Received request chatcmpl-13520592abe34950a0e75736949942ec: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:01:25 engine.py:267] Added request chatcmpl-13520592abe34950a0e75736949942ec.\n",
      "INFO 01-16 14:01:27 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:32 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:33 logger.py:37] Received request chatcmpl-2d1961d4c6e345c09d6fb5d25c47d16b: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:01:33 engine.py:267] Added request chatcmpl-2d1961d4c6e345c09d6fb5d25c47d16b.\n",
      "INFO 01-16 14:01:37 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:41 logger.py:37] Received request chatcmpl-57a22297dced420bad7a94ed0df79e55: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:01:41 engine.py:267] Added request chatcmpl-57a22297dced420bad7a94ed0df79e55.\n",
      "INFO 01-16 14:01:42 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:47 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:50 logger.py:37] Received request chatcmpl-57239e4471ca4c3cb96afd137d8144e8: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:01:50 engine.py:267] Added request chatcmpl-57239e4471ca4c3cb96afd137d8144e8.\n",
      "INFO 01-16 14:01:52 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:57 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:01:58 logger.py:37] Received request chatcmpl-9f3fa9d490ab46c78d7ed411f7485c1b: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:01:58 engine.py:267] Added request chatcmpl-9f3fa9d490ab46c78d7ed411f7485c1b.\n",
      "INFO 01-16 14:02:02 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:06 logger.py:37] Received request chatcmpl-c63ce6d3ff3f4ffebaf28ea25b22e1f4: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:02:07 engine.py:267] Added request chatcmpl-c63ce6d3ff3f4ffebaf28ea25b22e1f4.\n",
      "INFO 01-16 14:02:07 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:12 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:15 logger.py:37] Received request chatcmpl-f4702dc1e3e24477be1bd3dbd6e3b675: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:02:15 engine.py:267] Added request chatcmpl-f4702dc1e3e24477be1bd3dbd6e3b675.\n",
      "INFO 01-16 14:02:17 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:22 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:23 logger.py:37] Received request chatcmpl-99995520723c44f6b03dc2257006d204: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:02:23 engine.py:267] Added request chatcmpl-99995520723c44f6b03dc2257006d204.\n",
      "INFO 01-16 14:02:27 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:32 logger.py:37] Received request chatcmpl-98a750b5d6d849dc9aca5dc907bd36dc: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:02:32 engine.py:267] Added request chatcmpl-98a750b5d6d849dc9aca5dc907bd36dc.\n",
      "INFO 01-16 14:02:32 metrics.py:467] Avg prompt throughput: 85.8 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:37 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:40 logger.py:37] Received request chatcmpl-84a2b3c0f6f2462282b953ff24ffa703: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:02:40 engine.py:267] Added request chatcmpl-84a2b3c0f6f2462282b953ff24ffa703.\n",
      "INFO 01-16 14:02:42 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:47 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:48 logger.py:37] Received request chatcmpl-dc15ae611b6049b0a4e52caa2970ef0e: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:02:48 engine.py:267] Added request chatcmpl-dc15ae611b6049b0a4e52caa2970ef0e.\n",
      "INFO 01-16 14:02:52 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:02:57 logger.py:37] Received request chatcmpl-03f74586579d4eefade83191f905a03c: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:02:57 engine.py:267] Added request chatcmpl-03f74586579d4eefade83191f905a03c.\n",
      "INFO 01-16 14:02:57 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:02 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:05 logger.py:37] Received request chatcmpl-ac1825be355543ada362cd335fa0043f: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:03:05 engine.py:267] Added request chatcmpl-ac1825be355543ada362cd335fa0043f.\n",
      "INFO 01-16 14:03:07 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:12 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:13 logger.py:37] Received request chatcmpl-dcd0a165fc744990b0e2dc35a42b8e5d: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:03:13 engine.py:267] Added request chatcmpl-dcd0a165fc744990b0e2dc35a42b8e5d.\n",
      "INFO 01-16 14:03:17 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:22 logger.py:37] Received request chatcmpl-b0e0689a22a543f39cc18bc0bb1c4bf9: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:03:22 engine.py:267] Added request chatcmpl-b0e0689a22a543f39cc18bc0bb1c4bf9.\n",
      "INFO 01-16 14:03:22 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:27 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:30 logger.py:37] Received request chatcmpl-93f38a50b0a940288f49c8072c73996b: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:03:30 engine.py:267] Added request chatcmpl-93f38a50b0a940288f49c8072c73996b.\n",
      "INFO 01-16 14:03:33 metrics.py:467] Avg prompt throughput: 86.0 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:38 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:39 logger.py:37] Received request chatcmpl-ed77f3926b4540e4b3a2a34a14fb36f8: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:03:39 engine.py:267] Added request chatcmpl-ed77f3926b4540e4b3a2a34a14fb36f8.\n",
      "INFO 01-16 14:03:43 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:47 logger.py:37] Received request chatcmpl-3249643fad5d4e76865091dd0d95c2aa: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:03:47 engine.py:267] Added request chatcmpl-3249643fad5d4e76865091dd0d95c2aa.\n",
      "INFO 01-16 14:03:48 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:53 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:03:55 logger.py:37] Received request chatcmpl-81b1c4bf3f4c41a4825a231d7e6f3c97: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:03:55 engine.py:267] Added request chatcmpl-81b1c4bf3f4c41a4825a231d7e6f3c97.\n",
      "INFO 01-16 14:03:58 metrics.py:467] Avg prompt throughput: 85.9 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:03 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:04 logger.py:37] Received request chatcmpl-1d8b56f8d68540f380d8ebe6bb12fd6e: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:04:04 engine.py:267] Added request chatcmpl-1d8b56f8d68540f380d8ebe6bb12fd6e.\n",
      "INFO 01-16 14:04:08 metrics.py:467] Avg prompt throughput: 86.1 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:12 logger.py:37] Received request chatcmpl-fa8aabb04ce14db0bff1f4f3c7720d38: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:04:12 engine.py:267] Added request chatcmpl-fa8aabb04ce14db0bff1f4f3c7720d38.\n",
      "INFO 01-16 14:04:13 metrics.py:467] Avg prompt throughput: 86.2 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:18 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:20 logger.py:37] Received request chatcmpl-641f1afdf6f14c33a4bac6a88b66ad97: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:04:20 engine.py:267] Added request chatcmpl-641f1afdf6f14c33a4bac6a88b66ad97.\n",
      "INFO 01-16 14:04:23 metrics.py:467] Avg prompt throughput: 86.3 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:28 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:29 logger.py:37] Received request chatcmpl-ff5a15533920477ebb81b41c7ed7cb0f: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:04:29 engine.py:267] Added request chatcmpl-ff5a15533920477ebb81b41c7ed7cb0f.\n",
      "INFO 01-16 14:04:33 metrics.py:467] Avg prompt throughput: 85.3 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:37 logger.py:37] Received request chatcmpl-b990ae45ed944da7869756bcb6aedd9b: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:04:37 engine.py:267] Added request chatcmpl-b990ae45ed944da7869756bcb6aedd9b.\n",
      "INFO 01-16 14:04:38 metrics.py:467] Avg prompt throughput: 86.4 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:43 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:45 logger.py:37] Received request chatcmpl-add1a2fd1fbc45d29461a3668dae30a7: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:04:45 engine.py:267] Added request chatcmpl-add1a2fd1fbc45d29461a3668dae30a7.\n",
      "INFO 01-16 14:04:48 metrics.py:467] Avg prompt throughput: 85.3 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:53 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:04:54 logger.py:37] Received request chatcmpl-fbe3e462ccab4d4c9b44eda75b9e6152: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:04:54 engine.py:267] Added request chatcmpl-fbe3e462ccab4d4c9b44eda75b9e6152.\n",
      "INFO 01-16 14:04:58 metrics.py:467] Avg prompt throughput: 85.3 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:05:02 logger.py:37] Received request chatcmpl-bb4a3583f2f4464e8f6c6c3d8dd4310a: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:05:02 engine.py:267] Added request chatcmpl-bb4a3583f2f4464e8f6c6c3d8dd4310a.\n",
      "INFO 01-16 14:05:03 metrics.py:467] Avg prompt throughput: 85.3 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:05:08 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:05:10 logger.py:37] Received request chatcmpl-b3ef98a627bb412d9a92bf54a45f1618: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:05:10 engine.py:267] Added request chatcmpl-b3ef98a627bb412d9a92bf54a45f1618.\n",
      "INFO 01-16 14:05:13 metrics.py:467] Avg prompt throughput: 85.3 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:05:18 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:05:19 logger.py:37] Received request chatcmpl-67e27bac84794518a61fcc7db5e6a17d: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:05:19 engine.py:267] Added request chatcmpl-67e27bac84794518a61fcc7db5e6a17d.\n",
      "INFO 01-16 14:05:23 metrics.py:467] Avg prompt throughput: 85.3 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:05:27 logger.py:37] Received request chatcmpl-06188fa08e5e442fa0a800949469503d: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:05:27 engine.py:267] Added request chatcmpl-06188fa08e5e442fa0a800949469503d.\n",
      "INFO 01-16 14:05:28 metrics.py:467] Avg prompt throughput: 86.4 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:05:33 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 01-16 14:05:35 logger.py:37] Received request chatcmpl-a6185c1b3c0941eab00ec76479fffa92: prompt: '<|start_of_role|>user<|end_of_role|><s>[INST]<<SYS>>\\nYou are a helpful, respectful, and honest assistant. Always be as helpful as possible while being safe.\\nYou have access to several tools to assist you in completing tasks. You are responsible for determining when to use them and should break down complex tasks into subtasks if necessary.\\n\\nWhen using tools, follow this format:\\n\\n{\\n  \"thought\": \"Describe your thought process here, including why a tool may be necessary to proceed.\",\\n  \"action\": \"Specify the tool you want to use.\",\\n  \"action_input\": \"Provide the input for the tool.\"\\n}\\n\\nAfter performing an action, the tool will provide a response in the following format:\\n\\n{\\n  \"observation\": \"The result of the tool invocation\"\\n}\\n\\nYou should keep repeating the format (thought → action → observation) until you have the answer to the original question.\\n\\nIf the tool result is successful and the task is complete:\\n\\n{\\n  \"answer\": \"I have the answer: {tool_result}.\"\\n}\\n\\nOr, if you cannot answer:\\n\\n{\\n  \"answer\": \"Sorry, I cannot answer your query.\"\\n}\\n\\nRemember:\\n- Use the tools effectively and ensure inputs match the required format exactly as described.\\n- Maintain the JSON format and ensure all fields are filled out correctly.\\n- Do not include additional metadata such as `title`, `description`, or `type` in the `action_input`.\\n- If the tools give you a better or accurated response, use this immediately.\\nThe available tools include:\\n\\npython_repl - Use this to execute Python code. Input should be valid Python code as a string.\\n\\n<</SYS>>\\n\\n### QUESTION:\\nGive me how much are the daily 5000$ revenue per report minus 1500$ of cost per report. Obtain the revenue per year\\n\\n### ANSWER:\\n[/INST]\\n<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>', params: SamplingParams(n=1, presence_penalty=1.03, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32mINFO\u001b[0m:     ::1:42302 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "INFO 01-16 14:05:35 engine.py:267] Added request chatcmpl-a6185c1b3c0941eab00ec76479fffa92.\n",
      "INFO 01-16 14:05:38 metrics.py:467] Avg prompt throughput: 86.3 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.\n"
     ]
    }
   ],
   "source": [
    "!vllm serve \"ibm-granite/granite-3.0-8b-instruct\" --dtype auto --api-key alanliuxiang"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
