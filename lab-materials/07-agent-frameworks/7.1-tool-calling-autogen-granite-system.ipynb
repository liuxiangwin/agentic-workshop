{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58f224e-d375-4980-a5ce-ef8d80abedbe",
   "metadata": {},
   "source": [
    "## Tool Calling with Agentic AI - AutoGen\n",
    "\n",
    "### LLM Used - Granite3.0-8B\n",
    "\n",
    "In this notebook we will learn how to use Tool Calling with Agentic AI in order to solve different problems.\n",
    "\n",
    "Tool-calling agents expand the capabilities of an LLM by allowing it to interact with external systems. This approach empowers agents to dynamically solve problems by utilizing tools, accessing memory, and planning multi-step actions.\n",
    "\n",
    "Tool calling agents enable:\n",
    "\n",
    "1. Multi-Step Decision Making: The LLM can orchestrate a sequence of decisions to achieve complex objectives.\n",
    "2. Tool Access: The LLM can select and use various tools as needed to interact with external systems and APIs.\n",
    "\n",
    "This architecture allows for more dynamic and flexible behaviors, enabling agents to solve complex tasks by leveraging external resources efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47fe482b-147a-443a-9dc6-80668999d4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install autogen-agentchat~=0.2 autogen psutil --quiet\n",
    "# !pip install -q langchain-openai termcolor langchain_community duckduckgo_search wikipedia openapi-python-client==0.12.3 langgraph langchain_experimental yfinance\n",
    "!pip install -q langchain-openai termcolor langchain_community duckduckgo_search wikipedia openapi-python-client langgraph langchain_experimental openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c6bf30-660b-42ec-a2df-e2473af181b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patching name='PARAMS_MAPPING', member={'max_tokens': 'max_output_tokens', 'stop_sequences': 'stop_sequences', 'temperature': 'temperature', 'top_p': 'top_p', 'top_k': 'top_k', 'max_output_tokens': 'max_output_tokens'}, patched={'max_tokens': 'max_output_tokens', 'stop_sequences': 'stop_sequences', 'temperature': 'temperature', 'top_p': 'top_p', 'top_k': 'top_k', 'max_output_tokens': 'max_output_tokens'}\n",
      "Patching name='__init__', member=<function GeminiClient.__init__ at 0x7f5124c9a520>, patched=<function function.__call__ at 0x7f5124c9ae80>\n",
      "Patching name='_concat_parts', member=<function GeminiClient._concat_parts at 0x7f5124c9a8e0>, patched=<function function.__call__ at 0x7f5124c9af20>\n",
      "Patching name='_convert_json_response', member=<function GeminiClient._convert_json_response at 0x7f5124c9aa20>, patched=<function function.__call__ at 0x7f5124c9afc0>\n",
      "Patching name='_create_gemini_function_declaration', member=<function GeminiClient._create_gemini_function_declaration at 0x7f5124c9ab60>, patched=<function function.__call__ at 0x7f5124c9b060>\n",
      "Patching name='_create_gemini_function_declaration_schema', member=<function GeminiClient._create_gemini_function_declaration_schema at 0x7f5124c9ac00>, patched=<function function.__call__ at 0x7f5124c9b100>\n",
      "Patching name='_create_gemini_function_parameters', member=<function GeminiClient._create_gemini_function_parameters at 0x7f5124c9aca0>, patched=<function function.__call__ at 0x7f5124c9b1a0>\n",
      "Patching name='_initialize_vertexai', member=<function GeminiClient._initialize_vertexai at 0x7f5124c9a480>, patched=<function function.__call__ at 0x7f5124c9b240>\n",
      "Patching name='_oai_content_to_gemini_content', member=<function GeminiClient._oai_content_to_gemini_content at 0x7f5124c9a840>, patched=<function function.__call__ at 0x7f5124c9b2e0>\n",
      "Patching name='_oai_messages_to_gemini_messages', member=<function GeminiClient._oai_messages_to_gemini_messages at 0x7f5124c9a980>, patched=<function function.__call__ at 0x7f5124c9b380>\n",
      "Patching name='_to_json_or_str', member=<function GeminiClient._to_json_or_str at 0x7f5124c9ade0>, patched=<function function.__call__ at 0x7f5124c9b420>\n",
      "Patching name='_to_vertexai_safety_settings', member=<function GeminiClient._to_vertexai_safety_settings at 0x7f5124c9ad40>, patched=<function function.__call__ at 0x7f5124c9b4c0>\n",
      "Patching name='_tools_to_gemini_tools', member=<function GeminiClient._tools_to_gemini_tools at 0x7f5124c9aac0>, patched=<function function.__call__ at 0x7f5124c9b560>\n",
      "Patching name='cost', member=<function GeminiClient.cost at 0x7f5124c9a660>, patched=<function function.__call__ at 0x7f5124c9b6a0>\n",
      "Patching name='create', member=<function GeminiClient.create at 0x7f5124c9a7a0>, patched=<function function.__call__ at 0x7f5124c9b7e0>\n",
      "Patching name='get_usage', member=<function GeminiClient.get_usage at 0x7f5124c9a700>, patched=<function function.__call__ at 0x7f5124c9b880>\n",
      "Patching name='message_retrieval', member=<function GeminiClient.message_retrieval at 0x7f5124c9a5c0>, patched=<function function.__call__ at 0x7f5124c9b920>\n",
      "Patching name='__init__', member=<function LLMLingua.__init__ at 0x7f50abad91c0>, patched=<function function.__call__ at 0x7f50abad9120>\n",
      "Patching name='compress_text', member=<function LLMLingua.compress_text at 0x7f50abad9260>, patched=<function function.__call__ at 0x7f50abad93a0>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import autogen\n",
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "import autogen\n",
    "from autogen.cache import Cache\n",
    "\n",
    "# INFERENCE_SERVER_URL = os.getenv('API_URL_GRANITE')\n",
    "# MODEL_NAME = \"granite30-8b\"\n",
    "# API_KEY= os.getenv('API_KEY')\n",
    "\n",
    "INFERENCE_SERVER_URL = \"http://localhost:8000\"\n",
    "MODEL_NAME = \"ibm-granite/granite-3.0-8b-instruct\"\n",
    "API_KEY= \"alanliuxiang\"\n",
    "\n",
    "# INFERENCE_SERVER_URL = \"http://localhost:8089\"\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "# API_KEY= \"alanliuxiang\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576f3b77-9f04-4d3f-99aa-248f59f42b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the vLLM endpoint\n",
    "local_llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"api_key\": API_KEY,\n",
    "            \"base_url\": f\"{INFERENCE_SERVER_URL}/v1\"\n",
    "        }\n",
    "    ],\n",
    "    \"cache_seed\": None,\n",
    "    \"temperature\": 0.01,\n",
    "    \"timeout\": 600,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169003a0-1268-47d7-aa3b-7c831f452e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 01-28 12:52:03] {521} WARNING - Model ibm-granite/granite-3.0-8b-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "{'content': 'Agentic AI refers to artificial intelligence systems that are designed to act autonomously and make decisions on their own, rather than being controlled by humans. These systems are often equipped with advanced learning capabilities and can adapt to new situations and environments. They are used in various fields, including robotics, autonomous vehicles, and healthcare.', 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': []}\n"
     ]
    }
   ],
   "source": [
    "from autogen import ConversableAgent\n",
    "\n",
    "agent = ConversableAgent(\n",
    "    \"chatbot\",\n",
    "    llm_config=local_llm_config,\n",
    "    code_execution_config=False,  # Turn off code execution, by default it is off.\n",
    "    function_map=None,  # No registered functions, by default it is None.\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "reply = agent.generate_reply(messages=[{\"content\": \"What is Agentic AI?\", \"role\": \"user\"}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94aa0ba-8095-40af-9418-de0c88debd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autogen import UserProxyAgent, ConversableAgent\n",
    "\n",
    "\n",
    "chatbot = autogen.AssistantAgent(\n",
    "    name=\"chatbot\",\n",
    "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
    "    llm_config=local_llm_config,\n",
    ")\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    ")\n",
    "\n",
    "\n",
    "# define functions according to the function description\n",
    "\n",
    "\n",
    "# one way of registering functions is to use the register_for_llm and register_for_execution decorators\n",
    "@user_proxy.register_for_execution()\n",
    "@chatbot.register_for_llm(name=\"python\", description=\"run cell in ipython and return the execution result.\")\n",
    "def exec_python(cell: Annotated[str, \"Valid Python cell to execute.\"]) -> str:\n",
    "    ipython = get_ipython()\n",
    "    result = ipython.run_cell(cell)\n",
    "    log = str(result.result)\n",
    "    if result.error_before_exec is not None:\n",
    "        log += f\"\\n{result.error_before_exec}\"\n",
    "    if result.error_in_exec is not None:\n",
    "        log += f\"\\n{result.error_in_exec}\"\n",
    "    return log\n",
    "\n",
    "\n",
    "# another way of registering functions is to use the register_function\n",
    "def exec_sh(script: Annotated[str, \"Valid Python cell to execute.\"]) -> str:\n",
    "    return user_proxy.execute_code_blocks([(\"sh\", script)])\n",
    "\n",
    "\n",
    "autogen.agentchat.register_function(\n",
    "    exec_python,\n",
    "    caller=chatbot,\n",
    "    executor=user_proxy,\n",
    "    name=\"sh\",\n",
    "    description=\"run a shell script and return the execution result.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0ce2c2-c4e4-42fb-b2ee-b19f985fd635",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "Can you give me a program to check the space in my system in python? Then execute it\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 01-28 12:52:21] {521} WARNING - Model ibm-granite/granite-3.0-8b-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "To check the space in your system, you can use the `sh` function to run a shell command. Here's a simple Python program that uses the `df` command to display the amount of disk space used by the file system:\n",
      "\n",
      "```python\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"sh\",\n",
      "        \"parameters\": {\n",
      "            \"cell\": \"df -h\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "This command will display a table with information about the disk space usage, including the total, used, and available space for each mounted file system.\n",
      "\n",
      "Please note that the `sh` function is not available in the provided list of functions. However, you can use the `subprocess` module in Python to run shell commands. Here's how you can do it:\n",
      "\n",
      "```python\n",
      "import subprocess\n",
      "\n",
      "def check_disk_space():\n",
      "    result = subprocess.run(['df', '-h'], stdout=subprocess.PIPE)\n",
      "    return result.stdout.decode()\n",
      "\n",
      "print(check_disk_space())\n",
      "```\n",
      "\n",
      "This program will print the output of the `df -h` command, which displays the disk space usage in a human-readable format.\n",
      "\n",
      "Please replace the `sh` function with the `subprocess` module in the provided code.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "overlay         900G   53G  847G   6% /\n",
      "tmpfs            64M     0   64M   0% /dev\n",
      "tmpfs            37G  135M   37G   1% /etc/passwd\n",
      "tmpfs            57G  8.0K   57G   1% /dev/shm\n",
      "/dev/nvme0n1p4  900G   53G  847G   6% /etc/hosts\n",
      "/dev/nvme1n1    786G   45G  741G   6% /opt/app-root/src\n",
      "tmpfs            57G   24K   57G   1% /run/secrets/kubernetes.io/serviceaccount\n",
      "tmpfs            91G   12K   91G   1% /proc/driver/nvidia\n",
      "overlay         900G   53G  847G   6% /usr/bin/nvidia-smi\n",
      "tmpfs            37G  135M   37G   1% /usr/lib/firmware/nvidia/550.127.08/gsp_ga10x.bin\n",
      "tmpfs            37G  135M   37G   1% /usr/lib/firmware/nvidia/550.127.08/gsp_tu10x.bin\n",
      "tmpfs            64M     0   64M   0% /dev/nvidia0\n",
      "tmpfs            91G     0   91G   0% /proc/acpi\n",
      "tmpfs            91G     0   91G   0% /proc/scsi\n",
      "tmpfs            91G     0   91G   0% /sys/firmware\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 01-28 12:52:33] {521} WARNING - Model ibm-granite/granite-3.0-8b-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "The output of the `df -h` command shows the disk space usage for each mounted file system. In this case, the output shows that the `/` file system (which is the root file system) has 900G of total space, with 53G used and 847G available. The `Use%` column shows that the root file system is currently using 6% of its total space.\n",
      "\n",
      "The other file systems listed in the output are also mounted on the system, and their disk space usage is also displayed.\n",
      "\n",
      "The `df -h` command is a useful tool for checking the disk space usage on a Linux system. It can help you identify which file systems are using the most space, and can help you manage your disk space more effectively.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 01-28 12:52:36] {521} WARNING - Model ibm-granite/granite-3.0-8b-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "I'm glad I could help! If you have any other questions or need further assistance, please don't hesitate to ask.\n",
      "\n",
      "To terminate the conversation, you can say \"TERMINATE\".\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 01-28 12:52:39] {521} WARNING - Model ibm-granite/granite-3.0-8b-instruct is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "I'm here to help! If you have any other questions or need further assistance, please don't hesitate to ask.\n",
      "\n",
      "To terminate the conversation, you can say \"TERMINATE\".\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with Cache.disk() as cache:\n",
    "    # start the conversation\n",
    "    user_proxy.initiate_chat(\n",
    "        chatbot,\n",
    "        message=\"Can you give me a program to check the space in my system in python? Then execute it\",\n",
    "        cache=None,\n",
    "        max_turns=4,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65871f7-8612-46ad-b289-7e3e62610c38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 195175501824, 'available': 181163085824, 'percent': 7.2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "def check_memory():\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    return {\n",
    "        'total': memory_info.total,\n",
    "        'available': memory_info.available,\n",
    "        'percent': memory_info.percent\n",
    "    }\n",
    "\n",
    "check_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
