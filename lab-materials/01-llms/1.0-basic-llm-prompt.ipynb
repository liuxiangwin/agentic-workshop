{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "766aaa81-96e6-42dc-b29d-8216d2a7feec",
   "metadata": {},
   "source": [
    "## 1.0 - Basic LLM Prompting with vLLM and Langchain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0022f3fb-ee50-40f2-b276-b8194668e49e",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16ed2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.9.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install -q langchain==0.1.9 openai==1.13.3\n",
    "# !pip install langchain-openai\n",
    "#!pip uninstall langchain openai\n",
    "!pip install -q langchain-openai termcolor langchain_community duckduckgo_search wikipedia openapi-python-client langgraph langchain_experimental openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bb3f0f-40b5-49a6-b493-5e361db0113e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b908fd0-01dd-4ad2-b745-b3a4c56a7a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INFERENCE_SERVER_URL = os.getenv('API_URL')\n",
    "# MODEL_NAME = \"mistral-7b-instruct\"\n",
    "# API_KEY= os.getenv('API_KEY')\n",
    "INFERENCE_SERVER_URL = \"http://localhost:8080\"\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "API_KEY= \"alanliuxiang\"\n",
    "\n",
    "\n",
    "# INFERENCE_SERVER_URL = \"http://localhost:8000\"\n",
    "# MODEL_NAME = \"ibm-granite/granite-3.0-8b-instruct\"\n",
    "# API_KEY= \"alanliuxiang\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cfd95-02ce-4a3c-ae94-687a80a3c922",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "Langchain (https://www.langchain.com/) is a framework for developing applications powered by language models. It will take care for us of all the boilerplate code we would have to manually write to properly query an LLM.\n",
    "\n",
    "We will start by creating an **llm** instance, defined by the location where the LLM API can be queried and some parameters that will be applied to the model. For example, `max_new_tokens` will instruct the model to answer with a maximum of 512 tokens (words or parts of words). `temperature`, set really low here, will instruct the model to stay truth-grounded, and not try to be too \"creative\". After all, we're not trying to write a fancy poem here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b2f3f-ac23-4531-984b-6e8357233992",
   "metadata": {},
   "source": [
    "#### Create the LLM instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01baa2b8-529d-455d-ad39-ef4a96dbaf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM definition\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base= f\"{INFERENCE_SERVER_URL}/v1\",\n",
    "    model_name=MODEL_NAME,\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211bd1f-3e55-41ce-b3bc-0ee5db689c66",
   "metadata": {},
   "source": [
    "#### Create the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd8bc4c-b353-4a51-a8b7-6cb348e19623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe.\n",
    "You will be asked a question, to which you must give an answer.\n",
    "Your answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, answer \"I don't know\".\n",
    "<</SYS>>\n",
    "\n",
    "### QUESTION:\n",
    "{input}\n",
    "\n",
    "### ANSWER:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240fd72-f910-4ba4-82dc-c5ebf278452b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First Query to LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5cb39d-1639-4713-8423-4ea4925ef6d1",
   "metadata": {},
   "source": [
    "#### Create the Chain using the different components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f875c62b-931a-402e-9494-98b7296f8b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1782/595551351.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  conversation = LLMChain(llm=llm,\n"
     ]
    }
   ],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                            prompt=PROMPT,\n",
    "                            verbose=False,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969484f2-1f55-4021-a995-1d529cf9cc54",
   "metadata": {},
   "source": [
    "#### Let's talk..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee09847d-9ff7-4181-ad11-96186bf0a322",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The S&P 500 is a stock market index that tracks the performance of 500 large companies listed on stock exchanges in the United States. It is one of the most widely followed equity indices and is often used as a benchmark for the overall U.S. stock market. The index is market-capitalization weighted, meaning that larger companies have a greater influence on the index's performance."
     ]
    }
   ],
   "source": [
    "first_input = \"Describe what is the S&P500 in 100 words or less.\"\n",
    "conversation.predict(input=first_input);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53514c74-27fd-48f7-8f74-dcc96b4856bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### LLM without Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ed6c44-a016-49bd-8623-f231c5bba5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry for the confusion, but I need more context to provide an accurate answer. Could you please specify which companies or industry you're referring to?"
     ]
    }
   ],
   "source": [
    "second_input = \"How many companies are included?\"\n",
    "conversation.predict(input=second_input);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1224be-28a5-46b4-9fe1-db6e1676bd47",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "Let's head over to give our LLM Conversational Memory!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
